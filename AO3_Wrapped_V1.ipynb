{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Š AO3 Wrapped: Your Reading Year in Review\n",
        "\n",
        "Explore and visualize your personal reading history from Archive of Our Own â€” see your fandom trends, favorite works, and reading patterns throughout the year.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## âš ï¸ **Important:** Make a Copy First\n",
        "\n",
        "Before running this notebook, itâ€™s recommended to **make your own copy**:\n",
        "\n",
        "1. Click **File â†’ Save a copy in Drive**.\n",
        "2. Open the copy in your Google Drive.\n",
        "\n",
        "This ensures:\n",
        "\n",
        "- Your AO3 credentials and data stay private.\n",
        "- You can freely **customize settings** if you want.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## ğŸ“‘ Table of Contents\n",
        "\n",
        "1. **Setup** - Install dependencies and configure the environment.\n",
        "2. **Security & Privacy** - Important information about your data and credentials.\n",
        "3. **Login** - Authenticate with your AO3 account.\n",
        "4. **Data Collection** - Scrape your reading history or load existing data from CSV files.\n",
        "5. **Visualizations** â€” Your reading patterns at a glance:\n",
        "   - Summary stats\n",
        "   - Daily activity heatmap\n",
        "   - Deleted works over time\n",
        "   - Fandom trends\n",
        "   - Most visited works & authors\n",
        "   - Tag & fandom word clouds\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## âš™ï¸ Quick Start\n",
        "\n",
        "1. Run the **Setup** cell to install all requirements.\n",
        "2. Run the **Login** cell to authenticate with AO3.\n",
        "3. Choose to either **scrape new data** or **load existing data files**.\n",
        "4. Run the visualization cells to see your AO3 Wrapped!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5TIRQz1ZLnmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup\n",
        "Click the **Play button** (â–¶ï¸) on the code cell below to install all required packages and configure the environment. **You must run this cell before proceeding.**\n",
        "\n",
        "* This cell can take **1-2 minutes** to complete, especially on the first run.\n",
        "* It's installing necessary components like Python packages and special fonts for different languages.\n",
        "* Please wait for the **\"âœ… Setup complete!\"** message before moving on.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7V68j9-Mt23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SETUP & CONFIG: Install, Import, and Configure Environment (RUN ONCE)\n",
        "\"\"\"\n",
        "ğŸ”§ SETUP INSTRUCTIONS:\n",
        "1. Run this cell once (may take 30â€“60 seconds, or longer depending on network)\n",
        "2. Wait for the \"âœ… Setup complete!\" message\n",
        "3. Proceed to login and analysis cells\n",
        "\n",
        "ğŸ“‹ This cell:\n",
        "- Installs required Python packages\n",
        "- Installs Noto Sans CJK fonts (for CJK text support)\n",
        "- Imports all libraries\n",
        "- Configures matplotlib fonts with smart fallback\n",
        "- Loads analysis/visualization parameters\n",
        "\"\"\"\n",
        "\n",
        "print(\"Installing packages...\")\n",
        "\n",
        "# --- Package installation ---\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = {\n",
        "    \"beautifulsoup4\": \"bs4\",\n",
        "    \"requests\": \"requests\",\n",
        "    \"tqdm\": \"tqdm\",\n",
        "    \"seaborn\": \"seaborn\",\n",
        "    \"wordcloud\": \"wordcloud\",\n",
        "    \"Pillow\": \"PIL\",\n",
        "}\n",
        "\n",
        "for pip_name, import_name in packages.items():\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"  âœ“ {pip_name} already installed\")\n",
        "    except ImportError:\n",
        "        subprocess.check_call(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name]\n",
        "        )\n",
        "        print(f\"  âœ“ {pip_name} installed\")\n",
        "\n",
        "# --- Install CJK fonts ---\n",
        "print(\"Installing CJK fonts...\")\n",
        "subprocess.run(['apt-get', 'update', '-qq'],\n",
        "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['apt-get', 'install', '-y', 'fonts-noto-cjk'],\n",
        "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "print(\"  âœ“ Fonts installed\")\n",
        "\n",
        "# --- Import libraries ---\n",
        "print(\"Importing libraries...\")\n",
        "\n",
        "# Standard library\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import time\n",
        "import getpass\n",
        "from collections import Counter\n",
        "\n",
        "from google.colab import drive, files\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib import colormaps\n",
        "\n",
        "# Word cloud\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "\n",
        "print(\"  âœ“ Libraries imported\")\n",
        "\n",
        "# ========================================\n",
        "# âš™ï¸ CONFIGURATION SETTINGS (Analysis & Visualization)\n",
        "# ========================================\n",
        "\n",
        "# --- Scraping Settings ---\n",
        "YEAR_LIMIT = 2025\n",
        "SCRAPE_DELAY = 3  # seconds between requests\n",
        "MAX_PAGES=None\n",
        "\n",
        "# --- Visualization Settings ---\n",
        "NUM_TOP_FANDOMS = 8\n",
        "MAX_WORDCLOUD_WORDS = 100\n",
        "TITLE_FONT_SIZE = 22\n",
        "SUBTITLE_FONT_SIZE = 16\n",
        "PADDING = 50\n",
        "OUTPUT_DPI = 300\n",
        "\n",
        "# --- AO3 Color Palette ---\n",
        "AO3_BG_LIGHT = '#FFFFFF'\n",
        "AO3_TEXT_DARK = '#000000'\n",
        "AO3_ACCENT_RED = '#990000'\n",
        "AO3_GRID_LIGHT = '#EEEEEE'\n",
        "AO3_OTHER_GRAY = '#C0C0C0'\n",
        "AO3_GA = \"#94c90e\"\n",
        "AO3_TEEN = \"#ead70f\"\n",
        "AO3_MATURE = \"#eb7400\"\n",
        "AO3_EXPLICIT = '#a80403'\n",
        "\n",
        "# Custom colormap for heatmaps\n",
        "colors = [AO3_GRID_LIGHT, \"#FFC8C8\", \"#CC0000\", AO3_ACCENT_RED]\n",
        "AO3_HEATMAP_CMAP = LinearSegmentedColormap.from_list(\"AO3_Heatmap\", colors, N=256)\n",
        "\n",
        "# --- Global matplotlib settings (before font lock) ---\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'text.color': AO3_TEXT_DARK,\n",
        "    'axes.labelcolor': AO3_TEXT_DARK,\n",
        "    'xtick.color': AO3_TEXT_DARK,\n",
        "    'ytick.color': AO3_TEXT_DARK,\n",
        "    'figure.facecolor': AO3_BG_LIGHT,\n",
        "    'axes.facecolor': AO3_BG_LIGHT,\n",
        "    'axes.unicode_minus': False\n",
        "})\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# ğŸ”¤ FONT CONFIGURATION (Merged, Cached, Locked for Exports)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "print(\"Configuring fonts...\")\n",
        "\n",
        "_FONT_CACHE = None\n",
        "\n",
        "def setup_and_find_best_font():\n",
        "    \"\"\"\n",
        "    Install, register, and select the best available font.\n",
        "\n",
        "    Priority order:\n",
        "    1. Bold Noto Sans CJK\n",
        "    2. Regular Noto Sans CJK\n",
        "    3. Other Unicode-safe fonts (Noto Sans, Arial Unicode, DejaVu Sans)\n",
        "\n",
        "    Returns:\n",
        "        font_path (str | None): Path to selected font file\n",
        "        font_name (str): Font family name for matplotlib\n",
        "    \"\"\"\n",
        "    global _FONT_CACHE\n",
        "\n",
        "    if _FONT_CACHE is not None:\n",
        "        return _FONT_CACHE\n",
        "\n",
        "    # Register known Noto Sans CJK locations\n",
        "    possible_paths = [\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc\",\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\",\n",
        "        \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.otf\",\n",
        "        \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttf\",\n",
        "    ]\n",
        "\n",
        "    for p in possible_paths:\n",
        "        if os.path.exists(p):\n",
        "            fm.fontManager.addfont(p)\n",
        "\n",
        "    # Reload font manager to ensure discovery\n",
        "    fm._load_fontmanager(try_read_cache=False)\n",
        "\n",
        "    # Discover system fonts\n",
        "    fonts = (\n",
        "        fm.findSystemFonts(fontext=\"ttf\") +\n",
        "        fm.findSystemFonts(fontext=\"ttc\")\n",
        "    )\n",
        "\n",
        "    priority = [\n",
        "        \"notosanscjk\",\n",
        "        \"notosans\",\n",
        "        \"arialunicode\",\n",
        "        \"dejavusans\",\n",
        "    ]\n",
        "\n",
        "    fonts_lower = [f.lower() for f in fonts]\n",
        "\n",
        "    # Prefer bold\n",
        "    for key in priority:\n",
        "        for i, name in enumerate(fonts_lower):\n",
        "            if key in name and \"bold\" in name:\n",
        "                _FONT_CACHE = (fonts[i], fm.FontProperties(fname=fonts[i]).get_name())\n",
        "                break\n",
        "        if _FONT_CACHE is not None:\n",
        "            break\n",
        "\n",
        "    # Fallback to regular\n",
        "    if _FONT_CACHE is None:\n",
        "        for key in priority:\n",
        "            for i, name in enumerate(fonts_lower):\n",
        "                if key in name:\n",
        "                    _FONT_CACHE = (fonts[i], fm.FontProperties(fname=fonts[i]).get_name())\n",
        "                    break\n",
        "            if _FONT_CACHE is not None:\n",
        "                break\n",
        "\n",
        "    # Absolute fallback\n",
        "    if _FONT_CACHE is None:\n",
        "        if fonts:\n",
        "            _FONT_CACHE = (fonts[0], fm.FontProperties(fname=fonts[0]).get_name())\n",
        "        else:\n",
        "            _FONT_CACHE = (None, \"sans-serif\")\n",
        "\n",
        "    # Apply global matplotlib settings to lock typography\n",
        "    font_path, font_name = _FONT_CACHE\n",
        "    if font_path is not None:\n",
        "        prop = fm.FontProperties(fname=font_path)\n",
        "        plt.rcParams['font.family'] = prop.get_name()\n",
        "        plt.rcParams['font.sans-serif'] = [prop.get_name()]\n",
        "    else:\n",
        "        plt.rcParams['font.family'] = font_name\n",
        "        plt.rcParams['font.sans-serif'] = [font_name]\n",
        "\n",
        "    # Ensure PDFs and SVGs embed fonts for reproducibility\n",
        "    plt.rcParams['pdf.fonttype'] = 42  # TrueType\n",
        "    plt.rcParams['svg.fonttype'] = 'none'  # embed font paths\n",
        "\n",
        "    return _FONT_CACHE\n",
        "\n",
        "# Initialize font configuration\n",
        "FONT_PATH, FONT_NAME = setup_and_find_best_font()\n",
        "print(f\"  âœ“ Font configured: {FONT_NAME}\")\n",
        "\n",
        "print(\" Configuration loaded\")\n",
        "print(\"\\n âœ… Setup complete! You can now proceed to login and analysis.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gyyw_S9zG8Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Security & Privacy FAQ\n",
        "\n",
        "### Is it safe to enter my AO3 password?\n",
        "\n",
        "**Yes, when used as intended:**\n",
        "- This notebook runs **entirely in your private Google Colab session**\n",
        "- Your password is **never stored or logged**\n",
        "- Your credentials are only used to authenticate with AO3, then **cleared immediately**\n",
        "\n",
        "### Can the notebook creator see my password?\n",
        "\n",
        "**No:**\n",
        "- Each Colab session is **isolated to your account**\n",
        "- The creator has **no access** to your inputs, data, or runtime\n",
        "\n",
        "### What happens to my password after login?\n",
        "- Used once for authentication\n",
        "- **Deleted from memory right after**\n",
        "- Not saved to files, variables, or outputs\n",
        "\n",
        "### Still concerned?\n",
        "\n",
        "**You can:**\n",
        "1. Review the login code `ao3_login()`\n",
        "2. Use a temporary AO3 password\n",
        "3. Download and run the notebook locally\n",
        "\n",
        "### Security Best Practices\n",
        "- Clear outputs before sharing (`Edit > Clear all outputs`)\n",
        "- Use a strong, unique AO3 password\n"
      ],
      "metadata": {
        "id": "HuSp2GHzJ523"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Login"
      ],
      "metadata": {
        "id": "MKrirSl2NDvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log in with Password\n",
        "\n",
        "**ğŸ“ Instructions:**\n",
        "\n",
        "1. Click the â–¶ï¸ **Play button** on the left side of this cell.\n",
        "2. When prompted for **`AO3 Username:`**\n",
        "   - Type your username, then press **Enter**.\n",
        "3. When prompted for **`AO3 Password:`**\n",
        "   - Type your password, then press **Enter**.  \n",
        "   - *(Your password will be hidden for security.)*\n",
        "4. Wait for the **`âœ… Login Successful`** message.  \n",
        "   - This may take **up to 1-2 minutes**, depending on network speed.\n",
        "5. If login succeeds, a **link will appear in the output**.  \n",
        "   - Click it to confirm your AO3 page loads correctly.\n",
        "\n",
        "---\n",
        "\n",
        "### If login fails\n",
        "\n",
        "- Try waiting **10â€“15 minutes**, then run the cell again  *(shared Google Colab IPs may trigger temporary rate limits)*.\n",
        "- If the problem continues, click \"Show code\" and change  `LOG_LEVEL = \"DEBUG\"`  to see more detailed diagnostic messages.\n",
        "- You can also increase `REQUEST_TIMEOUT` if your network is slow.\n",
        "\n"
      ],
      "metadata": {
        "id": "oN0LkADxolBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Log in to AO3 with Password\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import getpass\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "LOG_LEVEL = \"USER\"   # USER | INFO | DEBUG\n",
        "REQUEST_TIMEOUT = 60\n",
        "\n",
        "# Prevent repeated login attempts in shared IP environments\n",
        "_LOGIN_ATTEMPTED = False\n",
        "\n",
        "# =========================\n",
        "# LOGGING\n",
        "# =========================\n",
        "def log(msg, level=\"INFO\"):\n",
        "    levels = {\"USER\": 0, \"INFO\": 1, \"DEBUG\": 2}\n",
        "    if levels[LOG_LEVEL] >= levels[level]:\n",
        "        print(msg)\n",
        "\n",
        "# =========================\n",
        "# COLAB-SAFE THROTTLING\n",
        "# =========================\n",
        "def polite_pause(min_s=2.5, max_s=5.0):\n",
        "    time.sleep(random.uniform(min_s, max_s))\n",
        "\n",
        "# =========================\n",
        "# RATE LIMIT DETECTION\n",
        "# =========================\n",
        "def is_rate_limited(response):\n",
        "    text = response.text.lower()\n",
        "    return (\n",
        "        response.status_code in (403, 429)\n",
        "        or \"retry later\" in text\n",
        "        or \"too many requests\" in text\n",
        "        or \"rate limit\" in text\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# DEBUG FILE SAVER (DEBUG ONLY)\n",
        "# =========================\n",
        "def save_debug_files(step1_resp=None, step2_resp=None, error=None, username=\"user\"):\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base = f\"ao3_debug_{username}_{timestamp}\"\n",
        "\n",
        "    if step1_resp:\n",
        "        with open(f\"{base}_step1_login_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(step1_resp.text)\n",
        "\n",
        "    if step2_resp:\n",
        "        with open(f\"{base}_step2_post_login.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(step2_resp.text)\n",
        "\n",
        "    if error:\n",
        "        with open(f\"{base}_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(error)\n",
        "\n",
        "    log(f\"ğŸ“ Debug files saved with prefix: {base}\", \"DEBUG\")\n",
        "\n",
        "# =========================\n",
        "# DETAILED DIAGNOSIS (DEBUG ONLY)\n",
        "# =========================\n",
        "def diagnose_login_failure(response):\n",
        "    issues = []\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    error_div = soup.find(\"div\", class_=\"error\")\n",
        "    if error_div:\n",
        "        issues.append(f\"AO3 error message: {error_div.get_text(strip=True)}\")\n",
        "\n",
        "    if \"/users/login\" in response.url:\n",
        "        issues.append(\"Redirected back to login page (credentials rejected)\")\n",
        "\n",
        "    if \"tos\" in response.url.lower():\n",
        "        issues.append(\"Terms of Service acceptance required\")\n",
        "\n",
        "    if \"cloudflare\" in response.text.lower():\n",
        "        issues.append(\"Cloudflare protection page detected\")\n",
        "\n",
        "    if is_rate_limited(response):\n",
        "        issues.append(\"Rate limiting detected\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "# =========================\n",
        "# MAIN LOGIN FUNCTION\n",
        "# =========================\n",
        "def ao3_login(username, password):\n",
        "    global _LOGIN_ATTEMPTED\n",
        "\n",
        "    if _LOGIN_ATTEMPTED:\n",
        "        log(\"âš ï¸ Login already attempted in this session.\", \"USER\")\n",
        "        log(\"Please wait 10â€“15 minutes before trying again.\", \"USER\")\n",
        "        return None\n",
        "\n",
        "    _LOGIN_ATTEMPTED = True\n",
        "\n",
        "    log(\n",
        "        \"â„¹ï¸ This notebook runs on shared Google Colab IPs. \"\n",
        "        \"If login fails, waiting before retrying usually helps.\",\n",
        "        \"USER\"\n",
        "    )\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "        ),\n",
        "        \"Referer\": \"https://archiveofourown.org/users/login\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    })\n",
        "\n",
        "    step1_resp = None\n",
        "    step2_resp = None\n",
        "\n",
        "    try:\n",
        "        # -------------------------\n",
        "        # Step 1: Fetch login page\n",
        "        # -------------------------\n",
        "        polite_pause()\n",
        "        step1_resp = session.get(\n",
        "            \"https://archiveofourown.org/users/login\",\n",
        "            timeout=REQUEST_TIMEOUT\n",
        "        )\n",
        "\n",
        "        if is_rate_limited(step1_resp):\n",
        "            log(\"âŒ AO3 is temporarily limiting requests from this IP.\", \"USER\")\n",
        "            log(\"Please wait 10â€“15 minutes and try again.\", \"USER\")\n",
        "            if LOG_LEVEL == \"DEBUG\":\n",
        "                save_debug_files(step1_resp=step1_resp, error=\"Rate limited\", username=username)\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(step1_resp.text, \"html.parser\")\n",
        "        token_el = soup.find(\"input\", {\"name\": \"authenticity_token\"})\n",
        "\n",
        "        if not token_el:\n",
        "            log(\"âŒ Login could not proceed due to AO3 security checks.\", \"USER\")\n",
        "            log(\"Tip: Log in once via your browser, then retry later.\", \"USER\")\n",
        "            if LOG_LEVEL == \"DEBUG\":\n",
        "                save_debug_files(step1_resp=step1_resp, error=\"Missing authenticity token\", username=username)\n",
        "            return None\n",
        "\n",
        "        token = token_el[\"value\"]\n",
        "\n",
        "        # -------------------------\n",
        "        # Step 2: Submit credentials\n",
        "        # -------------------------\n",
        "        polite_pause()\n",
        "        payload = {\n",
        "            \"utf8\": \"âœ“\",\n",
        "            \"authenticity_token\": token,\n",
        "            \"user[login]\": username,\n",
        "            \"user[password]\": password,\n",
        "            \"user[remember_me]\": \"1\",\n",
        "            \"commit\": \"Log in\",\n",
        "        }\n",
        "\n",
        "        step2_resp = session.post(\n",
        "            \"https://archiveofourown.org/users/login\",\n",
        "            data=payload,\n",
        "            timeout=REQUEST_TIMEOUT,\n",
        "            allow_redirects=True\n",
        "        )\n",
        "\n",
        "        # -------------------------\n",
        "        # Verify login success\n",
        "        # -------------------------\n",
        "        if \"/users/logout\" in step2_resp.text.lower():\n",
        "            log(f\"âœ… Login successful as {username}\", \"USER\")\n",
        "            return session\n",
        "\n",
        "        # -------------------------\n",
        "        # Failure (calm USER output)\n",
        "        # -------------------------\n",
        "        log(\"âŒ Login failed.\", \"USER\")\n",
        "        log(\n",
        "            \"Most common causes:\\n\"\n",
        "            \"â€¢ Incorrect username or password\\n\"\n",
        "            \"â€¢ AO3 rate limiting (shared IP)\\n\",\n",
        "            \"USER\"\n",
        "        )\n",
        "        log(\n",
        "            \"Tip: Wait 10â€“15 minutes or log in via browser once, then retry.\",\n",
        "            \"USER\"\n",
        "        )\n",
        "\n",
        "        if LOG_LEVEL == \"DEBUG\":\n",
        "            issues = diagnose_login_failure(step2_resp)\n",
        "            save_debug_files(\n",
        "                step1_resp,\n",
        "                step2_resp,\n",
        "                \" | \".join(issues) if issues else \"Unknown failure\",\n",
        "                username\n",
        "            )\n",
        "            log(\"ğŸ” DEBUG diagnosis:\", \"DEBUG\")\n",
        "            for issue in issues:\n",
        "                log(f\"   â€¢ {issue}\", \"DEBUG\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        log(\"âŒ AO3 request timed out. Please try again later.\", \"USER\")\n",
        "        return None\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        log(\"âŒ Network error. Check your internet connection.\", \"USER\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        log(\"âŒ An unexpected error occurred.\", \"USER\")\n",
        "        if LOG_LEVEL == \"DEBUG\":\n",
        "            save_debug_files(step1_resp, step2_resp, str(e), username)\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "# =========================\n",
        "# USER EXECUTION\n",
        "# =========================\n",
        "print(\"ğŸ” AO3 Login\\n\")\n",
        "\n",
        "if LOG_LEVEL == \"DEBUG\":\n",
        "    print(\"âš™ï¸ DEBUG MODE ENABLED â€” detailed files will be saved\\n\")\n",
        "\n",
        "try:\n",
        "    username = input(\"AO3 Username: \").strip()\n",
        "    password = getpass.getpass(\"AO3 Password (hidden): \")\n",
        "\n",
        "    if not username or not password:\n",
        "        print(\"âŒ Username and password are required.\")\n",
        "    else:\n",
        "        my_session = ao3_login(username, password)\n",
        "        if my_session:\n",
        "            print(\"\\nğŸ”‘ Authenticated session stored as `my_session`.\")\n",
        "            print(\"Example:\")\n",
        "            reading_url = f\"https://archiveofourown.org/users/{username}/readings\"\n",
        "            print(f\"session.get({reading_url})\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâš ï¸ Login cancelled by user.\")\n",
        "finally:\n",
        "    if 'password' in locals():\n",
        "        del password"
      ],
      "metadata": {
        "id": "JT4OoBptJVXY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;  \n",
        "&nbsp;  \n",
        "\n",
        "\n",
        "## Alternative way of log in: Cookies\n",
        "If logging in with a password fails, and you are already logged in on AO3, you can fetch your browser cookies to create a session and continue scraping. Here is how to find your `_otwarchive_session` and `cf_clearance` cookies for scraping:\n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ’» Desktop Browsers**\n",
        "\n",
        "### **Chrome / Edge**\n",
        "1. Log in to [AO3](https://archiveofourown.org).  \n",
        "2. Press `F12` (Windows) or `Cmd+Option+I` (Mac) â†’ **Developer Tools**.  \n",
        "3. Go to **Application â†’ Cookies â†’ https://archiveofourown.org**.  \n",
        "4. Find:  \n",
        "   - **`_otwarchive_session`** â†’ mandatory  \n",
        "   - **`cf_clearance`** â†’ optional\n",
        "5. Copy the **Value**. ğŸ”’ Keep secret.\n",
        "\n",
        "### **Firefox**\n",
        "1. Log in â†’ `F12` â†’ **Storage â†’ Cookies â†’ archiveofourown.org**  \n",
        "2. Copy `_otwarchive_session` (+ `cf_clearance` if needed).\n",
        "\n",
        "### **Safari (Mac)**\n",
        "1. Enable **Develop menu**: Safari â†’ Preferences â†’ Advanced â†’ â€œShow Develop menuâ€.  \n",
        "2. Develop â†’ Show Web Inspector â†’ Storage â†’ Cookies â†’ archiveofourown.org  \n",
        "3. Copy `_otwarchive_session` (+ `cf_clearance` if needed).\n",
        "\n",
        "---\n",
        "\n",
        "## **ğŸ“± Mobile Browsers**\n",
        "\n",
        "### **iOS Safari**\n",
        "1. Log in â†’ Settings â†’ Safari â†’ Advanced â†’ Website Data â†’ `archiveofourown.org`  \n",
        "2. Copy `_otwarchive_session` value. âš ï¸ Desktop is easier.\n",
        "\n",
        "### **Android Chrome / Firefox**\n",
        "1. Log in â†’ Use **cookie inspector extension** (e.g., EditThisCookie)  \n",
        "2. Or switch to desktop mode + remote debugging.  \n",
        "3. Copy `_otwarchive_session` (+ `cf_clearance` if needed).\n",
        "\n",
        "---\n",
        "\n",
        "## **âš ï¸ Tips**\n",
        "- ğŸ”’ **Never share your cookies** â€” full account access!  \n",
        "- â³ Cookies may **expire** after logout; refresh if scraping fails.  \n"
      ],
      "metadata": {
        "id": "BzWwoFwBbpCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Log in to AO3 - Option 2: Cookies\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import getpass\n",
        "\n",
        "def ao3_login_with_cookies(user_session_cookie, cf_clearance_cookie=None):\n",
        "    \"\"\"\n",
        "    Creates an authenticated AO3 session using browser cookies as an alternative to password login.\n",
        "\n",
        "    Args:\n",
        "        user_session_cookie (str): The value of the '_otwarchive_session' cookie from your browser.\n",
        "        cf_clearance_cookie (str, optional): The value of the 'cf_clearance' cookie,\n",
        "                                             which may be needed to bypass Cloudflare. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        requests.Session: An authenticated session object ready for scraping.\n",
        "    \"\"\"\n",
        "    if not user_session_cookie:\n",
        "        raise ValueError(\"The '_otwarchive_session' cookie value cannot be empty.\")\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                      \"(KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
        "    })\n",
        "\n",
        "    # Prepare the cookies for the session\n",
        "    cookies_to_set = {'_otwarchive_session': user_session_cookie}\n",
        "    if cf_clearance_cookie:\n",
        "        cookies_to_set['cf_clearance'] = cf_clearance_cookie\n",
        "\n",
        "    # Set the cookies on the session object\n",
        "    session.cookies.update(cookies_to_set)\n",
        "\n",
        "    return session\n",
        "\n",
        "\n",
        "# --- Start of login execution block ---\n",
        "print(\"ğŸ” AO3 Login with Cookies\\n\")\n",
        "print(\"Instead of a password, you will provide a session cookie from your browser.\")\n",
        "\n",
        "try:\n",
        "    # Prompt for the user's AO3 username\n",
        "    username = input(\"AO3 Username: \").strip()\n",
        "\n",
        "    # Securely prompt for the essential session cookie\n",
        "    user_session = getpass.getpass(\"Enter your '_otwarchive_session' cookie value (hidden): \")\n",
        "\n",
        "    # Prompt for the optional Cloudflare cookie\n",
        "    cf_clearance = getpass.getpass(\"Enter 'cf_clearance' cookie (optional, press Enter to skip): \")\n",
        "\n",
        "    if not username or not user_session:\n",
        "        print(\"\\nâŒ Username and the '_otwarchive_session' cookie are both required.\")\n",
        "    else:\n",
        "        print(f\"\\nğŸ”„ Creating session for '{username}' using cookies...\\n\")\n",
        "\n",
        "        # Create the session\n",
        "        my_session = ao3_login_with_cookies(user_session, cf_clearance or None)\n",
        "\n",
        "        # Verify login by accessing the history page\n",
        "        history_url = f\"https://archiveofourown.org/users/{username}/readings\"\n",
        "        try:\n",
        "            test_page = my_session.get(history_url, timeout=15)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"\\nâŒ Network error while accessing AO3: {e}\")\n",
        "        else:\n",
        "            # Check for login failure\n",
        "            if 'Log In' in test_page.text or test_page.status_code != 200:\n",
        "                print(\"âŒ Login Failed\\n\")\n",
        "                print(\"Possible reasons:\")\n",
        "                print(\"  â€¢ The '_otwarchive_session' cookie is incorrect or expired.\")\n",
        "                print(\"  â€¢ The 'cf_clearance' cookie might be required, or is also expired.\")\n",
        "                print(\"  â€¢ Tip: Log in to AO3 in your browser, refresh the page, and copy the latest cookie values.\")\n",
        "            else:\n",
        "                print(\"âœ… Login Successful! (Authenticated with cookies)\\n\")\n",
        "                print(f\"   Session created for user: {username}\")\n",
        "                print(\"   You can now proceed to scrape your history.\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâš ï¸ Login cancelled by user.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ An unexpected error occurred: {e}\")\n",
        "    print(\"   Please check your inputs and internet connection.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "De5IupOkbwej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Data Collection"
      ],
      "metadata": {
        "id": "KnZASY-SWq00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Scraping { display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ”„ Scrape Your Reading History\n",
        "#@markdown\n",
        "#@markdown **What this does:**\n",
        "#@markdown - Fetches your AO3 reading history page by page\n",
        "#@markdown - Extracts work details, tags, and statistics\n",
        "#@markdown - Tracks deleted works you've encountered\n",
        "#@markdown\n",
        "#@markdown **Progress:**\n",
        "#@markdown - A progress bar will show scraping status\n",
        "#@markdown - This may take 15-20 minutes for ~100 pages\n",
        "#@markdown - Please be patient and don't interrupt!\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def scrape_readings(session, username, year_limit=2025, delay=3, start_page=1, max_pages=None):\n",
        "    \"\"\"\n",
        "    Scrape AO3 reading history using an existing session object.\n",
        "\n",
        "    :param session: A requests.Session object that is already logged into AO3.\n",
        "    :param username: The AO3 username whose history is being scraped.\n",
        "    :param year_limit: Stop scraping when a work older than this year is encountered.\n",
        "    :param delay: Time (in seconds) to wait between page requests.\n",
        "    :param start_page: The page number to begin scraping from.\n",
        "    :param max_pages: Maximum number of pages to scrape, regardless of last_page.\n",
        "    :return: Tuple of (results, deleted_results)\n",
        "    \"\"\"\n",
        "\n",
        "    base_url = f\"https://archiveofourown.org/users/{username}/readings?page=\"\n",
        "\n",
        "    results = []\n",
        "    deleted_results = []\n",
        "    page = start_page\n",
        "    last_page = None\n",
        "    year_limit_reached = False\n",
        "    printed_first_work = False\n",
        "\n",
        "    # Initial page fetch to determine total pages\n",
        "    print(\" Determining total pages...\")\n",
        "    initial_url = base_url + str(start_page)\n",
        "    r = session.get(initial_url)\n",
        "\n",
        "    if r.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch initial page: HTTP {r.status_code}\")\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # Check for login/error page\n",
        "    page_title = soup.find('title').text.strip() if soup.find('title') else ''\n",
        "    if 'Log In' in page_title or 'Account Access' in page_title:\n",
        "        raise Exception(\"Session is invalid. Please log in again.\")\n",
        "\n",
        "    # Determine last page\n",
        "    nav = soup.find(\"ol\", role=\"navigation\")\n",
        "    if nav:\n",
        "        links = nav.find_all(\"a\")\n",
        "        if links and links[-1].text.strip() == 'Next â†’':\n",
        "            last_page = int(links[-2].text) if len(links) > 2 else 1\n",
        "        elif links:\n",
        "            try:\n",
        "                last_page = int(links[-1].text)\n",
        "            except ValueError:\n",
        "                last_page = 1\n",
        "        else:\n",
        "            last_page = 1\n",
        "    else:\n",
        "        last_page = 1\n",
        "\n",
        "    if max_pages is not None:\n",
        "        last_page = min(last_page, start_page + max_pages - 1)\n",
        "\n",
        "    print(f\" Reading History Found\")\n",
        "    print(f\"   If Year limit is not reached, scrape to page: {last_page}\")\n",
        "    print(f\"   Year limit: {year_limit}\")\n",
        "\n",
        "    # Create progress bar\n",
        "    pbar = tqdm(\n",
        "        range(start_page, last_page + 1),\n",
        "        desc=\"ğŸ“– Scraping pages\",\n",
        "        unit=\"page\",\n",
        "        ncols=100,\n",
        "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} pages [{elapsed}<{remaining}]'\n",
        "    )\n",
        "\n",
        "    # Process initial page (already fetched)\n",
        "    page_soups = {start_page: soup}\n",
        "\n",
        "    for page in pbar:\n",
        "        # Use already-fetched soup for first page, otherwise fetch new\n",
        "        if page not in page_soups:\n",
        "            url = base_url + str(page)\n",
        "            r = session.get(url)\n",
        "\n",
        "            if r.status_code == 429:\n",
        "                pbar.write(\" Rate-limited. Sleeping 60 seconds...\")\n",
        "                time.sleep(60)\n",
        "                r = session.get(url)  # Retry after waiting\n",
        "\n",
        "            if r.status_code != 200:\n",
        "                pbar.write(f\" Failed to fetch page {page}: HTTP {r.status_code}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        else:\n",
        "            soup = page_soups[page]\n",
        "\n",
        "        # Find works on page\n",
        "        work_list = soup.find(\"ol\", class_=\"reading work index group\") or soup.find(\"ol\", class_=\"work index group\")\n",
        "        works = work_list.find_all(\"li\", class_=re.compile(r\"work blurb group\")) if work_list else []\n",
        "\n",
        "        if not works:\n",
        "            pbar.write(f\" No works found on page {page}\")\n",
        "            break\n",
        "\n",
        "        # Update progress bar description with current stats\n",
        "        pbar.set_postfix({\n",
        "            'works': len(results),\n",
        "            'deleted': len(deleted_results)\n",
        "        })\n",
        "\n",
        "        for i, work in enumerate(works):\n",
        "            # Detect deleted works\n",
        "            classes = work.get(\"class\", [])\n",
        "            if any(\"deleted\" in c for c in classes):\n",
        "                viewed = work.find(\"h4\", class_=\"viewed heading\") or work.find(\"h4\", class_=\"viewed\")\n",
        "                raw_text = viewed.get_text(\" \", strip=True) if viewed else \"\"\n",
        "                m_deleted = re.search(r\"last visited\\s*([\\d]{1,2}\\s[A-Za-z]{3}\\s\\d{4})\", raw_text, re.IGNORECASE)\n",
        "                last_visited_date = m_deleted.group(1) if m_deleted else None\n",
        "\n",
        "                deleted_results.append({\n",
        "                    \"page\": page,\n",
        "                    \"index\": i,\n",
        "                    \"last_visited_date\": last_visited_date\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Work ID\n",
        "            work_id = None\n",
        "            work_id_attr = work.get('id')\n",
        "            if work_id_attr and work_id_attr.startswith('work_'):\n",
        "                try:\n",
        "                    work_id = int(work_id_attr.split('_')[-1])\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "            # Title & Author\n",
        "            h4 = work.find(\"h4\", class_=\"heading\")\n",
        "            if not h4 or not h4.a:\n",
        "                continue\n",
        "            title = h4.a.text.strip()\n",
        "            author_tag = h4.find(\"a\", rel=\"author\")\n",
        "            author = author_tag.text.strip() if author_tag else \"Anonymous\"\n",
        "\n",
        "            # Print the first non-deleted work once (for verification)\n",
        "            if not printed_first_work:\n",
        "                pbar.write(f\"âœ“ First work found: '{title}' by {author}\")\n",
        "                printed_first_work = True\n",
        "\n",
        "            # Fandoms\n",
        "            fandoms = []\n",
        "            h5 = work.find(\"h5\", class_=\"fandoms heading\")\n",
        "            if h5:\n",
        "                fandoms = [a.text.strip() for a in h5.find_all(\"a\", class_=\"tag\")]\n",
        "\n",
        "            # Rating & Category (from required-tags)\n",
        "            required_tags_ul = work.find(\"ul\", class_=\"required-tags\")\n",
        "            rating = None\n",
        "            category = None\n",
        "            if required_tags_ul:\n",
        "                rating_tag = required_tags_ul.find(\"span\", class_=re.compile(r\"rating-(.+)\"))\n",
        "                if rating_tag:\n",
        "                    rating = rating_tag.text.strip()\n",
        "                category_tag = required_tags_ul.find(\"span\", class_=re.compile(r\"category-(.+)\"))\n",
        "                if category_tag:\n",
        "                    category = category_tag.text.strip()\n",
        "\n",
        "                # Work completion status\n",
        "                complete_tag = required_tags_ul.find(\"span\", class_=re.compile(r\"complete-(yes|no)\"))\n",
        "                if complete_tag:\n",
        "                    complete_classes = complete_tag.get(\"class\", [])\n",
        "                    if \"complete-yes\" in complete_classes:\n",
        "                        is_complete = True\n",
        "                    elif \"complete-no\" in complete_classes:\n",
        "                        is_complete = False\n",
        "\n",
        "            # Tags (relationships, characters, freeforms, warnings)\n",
        "            tags_ul = work.find(\"ul\", class_=\"tags commas\")\n",
        "            relationships = []\n",
        "            characters = []\n",
        "            freeforms = []\n",
        "            warnings = []\n",
        "            if tags_ul:\n",
        "                for li in tags_ul.find_all(\"li\"):\n",
        "                    cls = li.get(\"class\", [])\n",
        "                    a_tag = li.find(\"a\", class_=\"tag\")\n",
        "                    if not a_tag:\n",
        "                        continue\n",
        "                    tag_text = a_tag.text.strip()\n",
        "                    if \"relationships\" in cls:\n",
        "                        relationships.append(tag_text)\n",
        "                    elif \"characters\" in cls:\n",
        "                        characters.append(tag_text)\n",
        "                    elif \"freeforms\" in cls:\n",
        "                        freeforms.append(tag_text)\n",
        "                    elif \"warnings\" in cls:\n",
        "                        warnings.append(tag_text)\n",
        "\n",
        "            # Summary text\n",
        "            # summary_text = None\n",
        "            # summary_blockquote = work.find(\"blockquote\", class_=\"userstuff summary\")\n",
        "            # if summary_blockquote:\n",
        "            #     paragraphs = summary_blockquote.find_all(\"p\")\n",
        "            #     summary_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
        "            #     if not summary_text:\n",
        "            #         summary_text = summary_blockquote.get_text(strip=True)\n",
        "\n",
        "            # Stats\n",
        "            stats_dl = work.find(\"dl\", class_=\"stats\")\n",
        "            def get_stat(class_name):\n",
        "                dd = stats_dl.find(\"dd\", class_=class_name) if stats_dl else None\n",
        "                if dd:\n",
        "                    text = dd.get_text(strip=True).replace(\",\", \"\")\n",
        "                    if text:\n",
        "                        try:\n",
        "                            return int(text)\n",
        "                        except ValueError:\n",
        "                            return 0\n",
        "                return 0\n",
        "\n",
        "            word_count = get_stat(\"words\")\n",
        "            hits = get_stat(\"hits\")\n",
        "            comments = get_stat(\"comments\")\n",
        "            kudos = get_stat(\"kudos\")\n",
        "            bookmarks = get_stat(\"bookmarks\")\n",
        "            language_dd = stats_dl.find(\"dd\", class_=\"language\") if stats_dl else None\n",
        "            language = language_dd.text.strip() if language_dd else \"Unknown\"\n",
        "\n",
        "            # Chapter information\n",
        "            chapters = None\n",
        "            chapters_dd = stats_dl.find(\"dd\", class_=\"chapters\") if stats_dl else None\n",
        "            if chapters_dd:\n",
        "                chapters = chapters_dd.text.strip()\n",
        "\n",
        "            # Visit info, year, and full date extraction\n",
        "            viewed_h4 = work.find(\"h4\", class_=\"viewed heading\")\n",
        "            if not viewed_h4:\n",
        "                viewed_h4 = work.find(\"h4\", class_=\"viewed\")\n",
        "            if not viewed_h4:\n",
        "                continue\n",
        "\n",
        "            viewed_text = viewed_h4.get_text(\" \", strip=True)\n",
        "\n",
        "            # Full read date\n",
        "            m_date_full = re.search(r\"Last visited[:\\s]*([\\d]{1,2}\\s[A-Za-z]{3}\\s[\\d]{4})\", viewed_text, re.IGNORECASE)\n",
        "            full_read_date = m_date_full.group(1).strip() if m_date_full else None\n",
        "\n",
        "            # Year extraction for limit check\n",
        "            if full_read_date:\n",
        "                m_year = re.search(r\"(\\d{4})$\", full_read_date)\n",
        "                year = int(m_year.group(1)) if m_year else None\n",
        "            else:\n",
        "                m_year_fallback = re.search(r\"\\b(20\\d{2})\\b\", viewed_text)\n",
        "                year = int(m_year_fallback.group(1)) if m_year_fallback else None\n",
        "\n",
        "            if year is None or year < year_limit:\n",
        "                pbar.write(f\"ğŸ“… Reached year limit (found work from {year})\")\n",
        "                year_limit_reached = True\n",
        "                break\n",
        "\n",
        "            # Visits count\n",
        "            m_visits = re.search(r\"Visited\\s(once|\\d+)\", viewed_text)\n",
        "            visited_count = 1 if (m_visits and m_visits.group(1) == \"once\") else (int(m_visits.group(1)) if m_visits else 0)\n",
        "\n",
        "            # Extract datetime\n",
        "            dt_tag = work.find(\"p\", class_=\"datetime\")\n",
        "            datetime_value = dt_tag.text.strip() if dt_tag else None\n",
        "\n",
        "            # Build work data\n",
        "            work_data = {\n",
        "                \"work_id\": work_id,\n",
        "                \"title\": title,\n",
        "                \"author\": author,\n",
        "                \"rating\": rating,\n",
        "                \"category\": category,\n",
        "                \"fandoms\": fandoms,\n",
        "                \"relationships\": relationships,\n",
        "                \"characters\": characters,\n",
        "                \"freeforms\": freeforms,\n",
        "                \"warnings\": warnings,\n",
        "                # \"summary\": summary_text,\n",
        "                \"is_complete\": is_complete,\n",
        "                \"chapters\": chapters,\n",
        "                \"word_count\": word_count,\n",
        "                \"visited_count\": visited_count,\n",
        "                \"language\": language,\n",
        "                \"hits\": hits,\n",
        "                \"comments\": comments,\n",
        "                \"kudos\": kudos,\n",
        "                \"bookmarks\": bookmarks,\n",
        "                \"year\": year,\n",
        "                \"full_read_date\": full_read_date,\n",
        "                \"last_updated\": datetime_value,\n",
        "                \"page\": page\n",
        "            }\n",
        "\n",
        "            results.append(work_data)\n",
        "\n",
        "        # Check if we broke out due to year limit\n",
        "        if year_limit_reached:\n",
        "            pbar.write(\"âœ“ Year limit reached, stopping scrape\")\n",
        "            break\n",
        "\n",
        "        # Add delay between pages (except for last page)\n",
        "        if page < last_page:\n",
        "            time.sleep(delay)\n",
        "\n",
        "    pbar.close()\n",
        "    return results, deleted_results\n",
        "\n",
        "\n",
        "# Execute scraping\n",
        "try:\n",
        "    print(f\" Starting scrape for user: {username}\")\n",
        "    print(f\" Year limit: {YEAR_LIMIT}\")\n",
        "    print(f\"â±  Delay between requests: {SCRAPE_DELAY}s\\n\")\n",
        "\n",
        "    results, deleted_results = scrape_readings(\n",
        "        session=my_session,\n",
        "        username=username,\n",
        "        year_limit=YEAR_LIMIT,\n",
        "        delay=SCRAPE_DELAY,\n",
        "        start_page=1,\n",
        "        max_pages=MAX_PAGES\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Scraping complete!\")\n",
        "    print(f\"    Collected {len(results)} works\")\n",
        "    print(f\"    Found {len(deleted_results)} deleted works\")\n",
        "\n",
        "    if len(results) == 0:\n",
        "        print(\"\\n WARNING: No works found. This might indicate:\")\n",
        "        print(\"   â€¢ Your reading history is empty for this year\")\n",
        "        print(\"   â€¢ There was a login or scraping issue\")\n",
        "        print(\"   â€¢ AO3 might be experiencing issues\")\n",
        "    # else:\n",
        "    #     # Show sample of collected data\n",
        "    #     print(f\"\\n Date range: {min(r['full_read_date'] for r in results)} to {max(r['full_read_date'] for r in results)}\")\n",
        "\n",
        "    # Create DataFrames\n",
        "    df = pd.DataFrame(results)\n",
        "    df_del = pd.DataFrame(deleted_results)\n",
        "\n",
        "    print(\"\\nâœ“ DataFrames created and ready for analysis!\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n Scraping interrupted by user\")\n",
        "    print(f\"   Partial results: {len(results)} works collected\")\n",
        "    df = pd.DataFrame(results) if results else pd.DataFrame()\n",
        "    df_del = pd.DataFrame(deleted_results) if deleted_results else pd.DataFrame()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Scraping failed: {e}\")\n",
        "    print(\"   Please check your internet connection and try again.\")\n",
        "    print(\"   If the problem persists, try logging in again.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "ET1bAVjKx12z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save to CSV File\n",
        "df.to_csv('ao3_history_2025.csv', index=False)\n",
        "df_del.to_csv('ao3_history_2025_deleted.csv', index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('ao3_history_2025.csv')\n",
        "files.download('ao3_history_2025_deleted.csv')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2xZpAphqRqu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data (SKIP IN 1ST RUN)\n",
        "\n",
        "If you have previously scraped your data and saved it, or have existing `ao3_history.csv` and `ao3_history_deleted.csv` files, you can load them here instead of re-scraping.\n",
        "\n",
        "You have two options for loading your files:\n",
        "### Option 1: Direct File Upload (Default)\n",
        "This is the easiest method. When you run the code cell below, two separate file upload dialogs will automatically appear.\n",
        "1.  **Run the code cell below.**\n",
        "2.  When the **first upload dialog** appears, select and upload your **ao3_history** file (e.g., `ao3_history_2025.csv`).\n",
        "3.  When the **second upload dialog** appears, select and upload your **ao3_history_deleted`** file.\n",
        "\n",
        "### Option 2: Load from Google Drive\n",
        "Choose this option if your CSV files are already stored in your Google Drive.\n",
        "1.  **Modify the code cell below:** Change the line `use_drive = False` to `use_drive = True`.\n",
        "2.  **Run the modified code cell.**\n",
        "3.  You will be prompted to connect to your Google Drive. Follow the instructions to authenticate.\n",
        "4.  Once connected, you'll be asked to **enter the full path** to your ao3_history file within your Google Drive (e.g., `/content/drive/MyDrive/my_ao3_data/ao3_history_2025.csv`).\n",
        "5.  Then, enter the full path to your `ao3_history_deleted.csv` file.\n",
        "\n",
        "**Important Notes:**\n",
        "*   Ensure the CSV files you use are the correct ones.\n",
        "*   The notebook expects two CSV files: one for your main reading history and one for deleted works."
      ],
      "metadata": {
        "id": "H8J6aBZgST7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Data (OPTIONAL)\n",
        "\"\"\"\n",
        "INSTRUCTIONS:\n",
        "1. Upload your CSV files to Google Drive\n",
        "2. Mount your drive below\n",
        "3. Update the file paths\n",
        "\n",
        "OR use the file upload option if you prefer\n",
        "\"\"\"\n",
        "# Option 1: Load from Google Drive\n",
        "use_drive = False  # Set to False to upload files directly\n",
        "\n",
        "if use_drive:\n",
        "    from google.colab import drive, files\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # UPDATE THESE PATHS to match your file locations\n",
        "    file_path = input(\"Enter path to ao3_history CSV: \").strip()\n",
        "    if not file_path:\n",
        "        print(\"please input file path\")\n",
        "\n",
        "    deleted_work_path = input(\"Enter path to deleted works CSV: \").strip()\n",
        "    if not deleted_work_path:\n",
        "        print(\"please input deleted work path\")\n",
        "else:\n",
        "    # Option 2: Upload files directly\n",
        "    print(\"Please upload your ao3_history CSV file:\")\n",
        "    uploaded = files.upload()\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "\n",
        "    print(\"Please upload your deleted works CSV file:\")\n",
        "    uploaded = files.upload()\n",
        "    deleted_work_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load the files\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df_del = pd.read_csv(deleted_work_path)\n",
        "    print(f\"âœ… Loaded {len(df)} works and {len(df_del)} deleted works\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ Error: File not found - {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading data: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z1UvcyedQScj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Visualization\n",
        "\n",
        "Run each code cell in this section individually to generate and display the different graphs and insights. Each visualization will also be saved automatically as a PNG image in your output folder."
      ],
      "metadata": {
        "id": "EKrKj6iqVHDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data Processing (MUST RUN before proceeding)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Helper function for consistent text in matplotlib\n",
        "# ------------------------------------------------------------------\n",
        "def add_text_locked(\n",
        "    obj,            # Figure or Axes\n",
        "    x, y,           # Position\n",
        "    text,           # Text string\n",
        "    fontsize=14,\n",
        "    fontweight=\"normal\",\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha=\"left\",\n",
        "    va=\"center\",\n",
        "    style=\"normal\",\n",
        "    alpha=1.0,\n",
        "    is_title=False,  # True if calling for ax.set_title\n",
        "    pad=0            # Optional padding for titles\n",
        "):\n",
        "    \"\"\"\n",
        "    Add text to a matplotlib Figure or Axes using the locked font.\n",
        "\n",
        "    Parameters:\n",
        "        obj: matplotlib.figure.Figure or matplotlib.axes.Axes\n",
        "        x, y: Position (fraction for fig, axes coords for ax)\n",
        "        text: Text string\n",
        "        fontsize, fontweight, color, ha, va, style, alpha: Text style\n",
        "        is_title: If True and obj is Axes, calls ax.set_title()\n",
        "        pad: Title padding\n",
        "    \"\"\"\n",
        "    prop = fm.FontProperties(fname=FONT_PATH)\n",
        "\n",
        "    if is_title and isinstance(obj, plt.Axes):\n",
        "        obj.set_title(\n",
        "            text,\n",
        "            fontproperties=prop,\n",
        "            fontsize=fontsize,\n",
        "            fontweight=fontweight,\n",
        "            color=color,\n",
        "            pad=pad,\n",
        "            loc=ha  # 'left', 'center', 'right'\n",
        "        )\n",
        "    elif isinstance(obj, plt.Axes):\n",
        "        obj.text(\n",
        "            x, y, text,\n",
        "            transform=obj.transAxes,\n",
        "            fontsize=fontsize,\n",
        "            fontweight=fontweight,\n",
        "            color=color,\n",
        "            ha=ha,\n",
        "            va=va,\n",
        "            fontproperties=prop,\n",
        "            style=style,\n",
        "            alpha=alpha\n",
        "        )\n",
        "    elif isinstance(obj, plt.Figure):\n",
        "        obj.text(\n",
        "            x, y, text,\n",
        "            fontsize=fontsize,\n",
        "            fontweight=fontweight,\n",
        "            color=color,\n",
        "            ha=ha,\n",
        "            va=va,\n",
        "            fontproperties=prop,\n",
        "            style=style,\n",
        "            alpha=alpha\n",
        "        )\n",
        "    else:\n",
        "        raise TypeError(\"obj must be a Figure or Axes\")\n",
        "\n",
        "\n",
        "# --- 1. Reusable Data Processing Utilities ---\n",
        "\n",
        "def get_daily_data(df, value_col, agg_func='count'):\n",
        "    \"\"\"Aggregates a metric daily. Uses 'full_read_date' index.\"\"\"\n",
        "\n",
        "    # Ensure date column is the index for aggregation\n",
        "    if 'full_read_date' not in df.columns:\n",
        "        df = df.reset_index(names='full_read_date') # assuming date is in index\n",
        "\n",
        "    df['full_read_date'] = pd.to_datetime(df['full_read_date'])\n",
        "    daily_data = df.groupby('full_read_date').agg(\n",
        "        daily_metric=(value_col, agg_func)\n",
        "    )['daily_metric']\n",
        "\n",
        "    return daily_data\n",
        "\n",
        "def reindex_time_series(daily_series):\n",
        "    \"\"\"Fills in missing dates within the range with 0s.\"\"\"\n",
        "    idx = daily_series.index\n",
        "    full_date_range = pd.date_range(start=idx.min(), end=idx.max(), freq='D')\n",
        "\n",
        "    # Reindex and fill NaN with 0\n",
        "    reindexed_series = daily_series.reindex(full_date_range, fill_value=0)\n",
        "    reindexed_series.index.name = 'full_read_date'\n",
        "\n",
        "    # Add calendar components needed for pivot\n",
        "    df_reindexed = reindexed_series.to_frame(name='daily_metric')\n",
        "    df_reindexed['dayofweek'] = df_reindexed.index.dayofweek # 0=Mon, 6=Sun\n",
        "    df_reindexed['weekofyear'] = df_reindexed.index.isocalendar().week.astype(int)\n",
        "\n",
        "    return df_reindexed\n",
        "\n",
        "def get_month_annotations(date_index):\n",
        "    \"\"\"Calculates X-positions and labels for month annotations.\"\"\"\n",
        "    month_starts = []\n",
        "    month_labels = []\n",
        "\n",
        "    first_date = date_index.min()\n",
        "    last_date = date_index.max()\n",
        "\n",
        "    # Find the starting week number of the dataset (needed for column index calculation)\n",
        "    min_week = first_date.isocalendar().week\n",
        "\n",
        "    # Iterate through every month in the data range\n",
        "    for month in pd.date_range(start=first_date, end=last_date, freq='MS'):\n",
        "        start_week = month.isocalendar().week\n",
        "\n",
        "        # Column index relative to the first week of the heatmap\n",
        "        col_index = start_week - min_week\n",
        "\n",
        "        month_starts.append(col_index + 0.5)\n",
        "        month_labels.append(month.strftime('%b'))\n",
        "\n",
        "    return month_starts, month_labels\n",
        "\n",
        "def get_monthly_top_categories(df, category_col, num_top=3, other_label='Other'):\n",
        "    \"\"\"\n",
        "    Identifies the top N categories WITHIN EACH MONTH.\n",
        "    Returns the actual top performers for each month, with all other categories grouped as 'Other'.\n",
        "    Excludes empty/null tags.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "    # --- Robustness Check: Ensure 'full_read_date' is a column, not the index ---\n",
        "    if isinstance(df_temp.index, pd.DatetimeIndex) and df_temp.index.name == 'full_read_date':\n",
        "        df_temp = df_temp.reset_index()\n",
        "\n",
        "    # 1. Prepare Data and Explode\n",
        "    # Ensure 'full_read_date' is datetime type (re-casting just in case)\n",
        "    df_temp['full_read_date'] = pd.to_datetime(df_temp['full_read_date'])\n",
        "\n",
        "    # Create the 'month' Period column\n",
        "    df_temp['month'] = df_temp['full_read_date'].dt.to_period('M')\n",
        "\n",
        "    # Explode the list-like column\n",
        "    df_exploded = df_temp.explode(category_col)\n",
        "    # Filter out null, empty string, or list-like empty tags\n",
        "    df_exploded = df_exploded[df_exploded[category_col].astype(str).str.strip().ne('')]\n",
        "    df_exploded = df_exploded.dropna(subset=[category_col])\n",
        "\n",
        "    # 2. Calculate Monthly Counts\n",
        "    monthly_counts = df_exploded.groupby(['month', category_col]).agg(\n",
        "        fic_count=('work_id', 'nunique')\n",
        "    ).reset_index()\n",
        "\n",
        "    # 3. Determine Top N Categories FOR EACH MONTH\n",
        "    def get_top_n(group, month_val):\n",
        "        # Sort by fic_count to identify the top N categories for this month\n",
        "        sorted_group = group.sort_values(by='fic_count', ascending=False)\n",
        "        top_n_categories = sorted_group.head(num_top)[category_col].tolist()\n",
        "\n",
        "        # Get the top N rows\n",
        "        top_n_group = sorted_group.head(num_top).copy()\n",
        "\n",
        "        # Calculate the sum of all categories NOT in the top N for this month\n",
        "        not_in_top = ~group[category_col].isin(top_n_categories)\n",
        "        other_sum = group.loc[not_in_top, 'fic_count'].sum()\n",
        "\n",
        "        # Create the 'Other' row if there are other categories\n",
        "        if other_sum > 0:\n",
        "            other_row = pd.DataFrame({\n",
        "                'month': [month_val],\n",
        "                category_col: [other_label],\n",
        "                'fic_count': [other_sum]\n",
        "            })\n",
        "            # Combine the monthly top N with the monthly 'Other' sum\n",
        "            top_n_group = pd.concat([top_n_group, other_row], ignore_index=True)\n",
        "\n",
        "        return top_n_group\n",
        "\n",
        "    # Apply the ranking/grouping function\n",
        "    final_monthly_data = monthly_counts.groupby('month', group_keys=False).apply(\n",
        "        lambda group: get_top_n(group, group.name)\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # 4. Get all unique categories that appear in top N across any month\n",
        "    all_top_categories = final_monthly_data[\n",
        "        final_monthly_data[category_col] != other_label\n",
        "    ][category_col].unique().tolist()\n",
        "\n",
        "    # 5. Pivot for the stacked chart format\n",
        "    pivot_data = final_monthly_data.pivot(\n",
        "        index='month',\n",
        "        columns=category_col,\n",
        "        values='fic_count'\n",
        "    ).fillna(0)\n",
        "\n",
        "    return pivot_data, all_top_categories\n",
        "\n",
        "# --------- TAGS PROCESSING --------------------------\n",
        "def safe_parse_list(x):\n",
        "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
        "        return []\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return x.tolist()\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if x in ('', '[]'):\n",
        "            return []\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "\n",
        "def clean_fandom(fandom_str):\n",
        "    \"\"\"\n",
        "    Clean a single fandom string to extract the core fandom name.\n",
        "\n",
        "    Examples:\n",
        "        \"å…¨èŒé«˜æ‰‹ - è´è¶è“ | QuÃ¡nzhÃ­ GÄoshÇ’u - HÃºdiÃ© LÃ¡n\" -> \"å…¨èŒé«˜æ‰‹\"\n",
        "        \"å…¨èŒé«˜æ‰‹ | The King's Avatar (Cartoon)\" -> \"å…¨èŒé«˜æ‰‹\"\n",
        "        \"å´©åï¼šæ˜Ÿç©¹é“é“ | Honkai: Star Rail (Video Game)\" -> \"å´©åï¼šæ˜Ÿç©¹é“é“\"\n",
        "        \"Original Work\" -> \"Original Work\"\n",
        "        \"åŸåˆ›ä½œå“\" -> \"Original Work\"\n",
        "    \"\"\"\n",
        "    if not fandom_str:\n",
        "        return None\n",
        "\n",
        "    # Standardize Original Work variations first\n",
        "    if fandom_str in ('åŸåˆ›ä½œå“', 'Original Works'):\n",
        "        return 'Original Work'\n",
        "\n",
        "    # Split on | to separate bilingual parts\n",
        "    parts = [p.strip() for p in fandom_str.split('|')]\n",
        "\n",
        "    # Take the first part (usually the original language)\n",
        "    core = parts[0]\n",
        "\n",
        "    # Remove everything after \" - \" (author/source info)\n",
        "    # Example: \"å…¨èŒé«˜æ‰‹ - è´è¶è“\" -> \"å…¨èŒé«˜æ‰‹\"\n",
        "    if ' - ' in core:\n",
        "        core = core.split(' - ')[0].strip()\n",
        "\n",
        "    # Remove parenthetical info (Video Game, TV, Cartoon, etc.)\n",
        "    core = re.sub(r'\\s*\\([^\\)]*\\)$', '', core).strip()\n",
        "\n",
        "    return core if core else None\n",
        "\n",
        "\n",
        "def clean_tag_generic(tag_str):\n",
        "    \"\"\"Standard cleaner for Characters, Relationships, and Freeforms.\"\"\"\n",
        "    if not tag_str or pd.isna(tag_str): return None\n",
        "    # Remove AO3 suffixes and parentheticals\n",
        "    tag_str = re.sub(r'\\s*-\\s*(Character|è§’è‰²|Relationship|Freeform)$', '',\n",
        "                     str(tag_str), flags=re.IGNORECASE)\n",
        "    tag_str = re.sub(r'\\s*\\([^\\)]*\\)$', '', tag_str).strip()\n",
        "    return tag_str if len(tag_str) > 1 else None\n",
        "\n",
        "\n",
        "def get_long_form(df, column_name, clean_func=None):\n",
        "    \"\"\"\n",
        "    1. Parses the raw data (handles strings/lists/NaNs).\n",
        "    2. Cleans and deduplicates items within each work.\n",
        "    3. Explodes into a long-form DataFrame.\n",
        "    \"\"\"\n",
        "    processed_rows = []\n",
        "\n",
        "    # We only need these columns for the final analysis\n",
        "    essential_cols = ['work_id', 'rating', 'full_read_date']\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Step 1: Safe parse to get a real Python list\n",
        "        raw_list = safe_parse_list(row.get(column_name, []))\n",
        "\n",
        "        # Step 2: Clean and Deduplicate within this specific work\n",
        "        cleaned_set = set()\n",
        "        for item in raw_list:\n",
        "            cleaned_item = clean_func(item) if clean_func else item\n",
        "            if cleaned_item:\n",
        "                cleaned_set.add(cleaned_item)\n",
        "\n",
        "        # Step 3: Create a row for each unique item\n",
        "        for item in cleaned_set:\n",
        "            processed_rows.append({\n",
        "                'work_id': row.get('work_id'),\n",
        "                'full_read_date': row.get('full_read_date'),\n",
        "                'rating': row.get('rating'),\n",
        "                'attribute_value': item\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(processed_rows)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Helper: auto-tune max_words\n",
        "# =========================================================\n",
        "def choose_max_words(freq_dict, width=1600, height=800, coverage_target=0.85):\n",
        "    counts = pd.Series(freq_dict).sort_values(ascending=False)\n",
        "\n",
        "    coverage_limit = (\n",
        "        (counts.cumsum() / counts.sum() <= coverage_target)\n",
        "        .sum()\n",
        "    )\n",
        "\n",
        "    area_limit = int((width * height) / 20000)\n",
        "\n",
        "    return max(15, min(coverage_limit, area_limit))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Helper: auto-tune margin\n",
        "# =========================================================\n",
        "def choose_margin(max_words, min_margin=4, max_margin=14):\n",
        "    min_words, max_words_ref = 20, 120\n",
        "    mw = np.clip(max_words, min_words, max_words_ref)\n",
        "\n",
        "    margin = min_margin + (max_margin - min_margin) * (\n",
        "        (mw - min_words) / (max_words_ref - min_words)\n",
        "    )\n",
        "\n",
        "    return int(round(margin))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Helper: CJK-safe font selection\n",
        "# =========================================================\n",
        "def find_best_font():\n",
        "    fonts = (\n",
        "        font_manager.findSystemFonts(fontext=\"ttf\") +\n",
        "        font_manager.findSystemFonts(fontext=\"ttc\")\n",
        "    )\n",
        "\n",
        "    priority = [\n",
        "        \"notosanscjk\",\n",
        "        \"notosans\",\n",
        "        \"arialunicode\",\n",
        "        \"dejavusans\",\n",
        "    ]\n",
        "\n",
        "    fonts_lower = [f.lower() for f in fonts]\n",
        "\n",
        "    # Prefer bold\n",
        "    for key in priority:\n",
        "        for i, name in enumerate(fonts_lower):\n",
        "            if key in name and \"bold\" in name:\n",
        "                return fonts[i]\n",
        "\n",
        "    # Fallback to regular\n",
        "    for key in priority:\n",
        "        for i, name in enumerate(fonts_lower):\n",
        "            if key in name:\n",
        "                return fonts[i]\n",
        "\n",
        "    return fonts[0] if fonts else None\n",
        "\n",
        "\n",
        "# ---------- WordCloud Generation ----------\n",
        "def generate_wc(freq_dict, colors=\"Reds\", custom_mask=None, stop_words=None):\n",
        "    \"\"\"\n",
        "    Generate a WordCloud object from a frequency dictionary with optional customization.\n",
        "\n",
        "    This function creates a WordCloud using frequencies of terms provided in `freq_dict`.\n",
        "    It supports stop word filtering, custom masks, color maps, and auto-tuned parameters\n",
        "    for maximum words, margins, and font selection.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_dict : dict\n",
        "        A dictionary where keys are words/phrases and values are their frequencies.\n",
        "        Example: {'Harry': 50, 'Hermione': 40, 'Ron': 35}\n",
        "\n",
        "    colors : str, optional (default=\"Reds\")\n",
        "        The Matplotlib colormap to use for the word colors.\n",
        "        Can also accept a list of colors.\n",
        "\n",
        "    custom_mask : array-like or None, optional (default=None)\n",
        "        A 2D numpy array or image to shape the word cloud.\n",
        "        Non-zero values indicate where words can be drawn.\n",
        "\n",
        "    stop_words : set or list of str, optional (default=None)\n",
        "        Words to exclude from the word cloud.\n",
        "        Comparison is case-insensitive.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    WordCloud\n",
        "        A WordCloud object from the `wordcloud` library, ready for plotting or saving.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - The function uses helper functions `choose_max_words`, `choose_margin`, and `find_best_font`\n",
        "      to automatically adjust layout parameters based on the frequency dictionary.\n",
        "    - Collocations are disabled to avoid multi-word combinations.\n",
        "    - Relative scaling is set to 0.3 to balance size differences between frequent and less frequent words.\n",
        "    - Higher scale (3) is used for better rendering resolution.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> freq_dict = {'Harry': 50, 'Hermione': 40, 'Ron': 35}\n",
        "    >>> wc = generate_wc(freq_dict, colors=\"Blues\", stop_words={\"the\", \"and\"})\n",
        "    >>> plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    >>> plt.axis(\"off\")\n",
        "    >>> plt.show()\n",
        "    \"\"\"\n",
        "    # Filter stop words\n",
        "    if stop_words:\n",
        "        freq_dict = {k: v for k, v in freq_dict.items() if k.lower() not in stop_words}\n",
        "\n",
        "    # Use auto-tuning helpers\n",
        "    max_words = choose_max_words(freq_dict)\n",
        "    margin = choose_margin(max_words)\n",
        "    font_path = find_best_font()\n",
        "\n",
        "    wc = WordCloud(\n",
        "        background_color=AO3_BG_LIGHT,\n",
        "        max_words=max_words,\n",
        "        scale=3,  # Higher scale for better resolution\n",
        "        margin=margin,\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        font_path=font_path,\n",
        "        mask=custom_mask,\n",
        "        colormap=colors,\n",
        "        collocations=False,\n",
        "        relative_scaling=0.3\n",
        "    )\n",
        "\n",
        "    wc.generate_from_frequencies(freq_dict)\n",
        "    return wc\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Reusable Data Processing Utilities ---\n",
        "# 1. Prepare cleaners\n",
        "cleaners = {\n",
        "    'fandoms': clean_fandom,\n",
        "    'relationships': clean_tag_generic,\n",
        "    'characters': clean_tag_generic,\n",
        "    'freeforms': clean_tag_generic\n",
        "}\n",
        "\n",
        "# 2. Generate the cleaned long-form dataframes\n",
        "cleaned_data = {}\n",
        "for col, func in cleaners.items():\n",
        "    cleaned_data[col] = get_long_form(df, col, clean_func=func)"
      ],
      "metadata": {
        "id": "9hDmRzbjaV7S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 1: Summary Stats\n",
        "\n"
      ],
      "metadata": {
        "id": "jwksFUnaZVOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title AO3 Wrapped Cover Page\n",
        "\n",
        "# --- 1. Quick Stats Calculation ---\n",
        "total_works = len(df)\n",
        "total_words = df['word_count'].sum()\n",
        "df['word_count'] = pd.to_numeric(df['word_count'], errors='coerce')\n",
        "longest_fic_words = int(df['word_count'].max())\n",
        "df['last_updated_dt'] = pd.to_datetime(df['last_updated'], format='%d %b %Y', errors='coerce')\n",
        "oldest_fic_year = df['last_updated_dt'].min().strftime('%b %Y')\n",
        "\n",
        "# --- 2. Main Color Settings ---\n",
        "AO3_BG_LIGHT = '#FFFFFF'\n",
        "AO3_TEXT_DARK = '#000000'\n",
        "AO3_ACCENT_RED = '#990000'\n",
        "AO3_GRID_LIGHT = '#EEEEEE'\n",
        "AO3_OTHER_GRAY = '#C0C0C0'\n",
        "RATING_COLORS = ['#94c90e', '#ead70f', '#eb7400', '#a80403'] # GA, TEEN, MATURE, EXPLICIT\n",
        "\n",
        "# --- 3. Canvas setup (1:1 ratio) ---\n",
        "fig = plt.figure(figsize=(12, 12), facecolor=AO3_BG_LIGHT)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# --- 4. Background Accents ---\n",
        "# Top brand bar\n",
        "ax.add_patch(plt.Rectangle((0, 0.94), 1, 0.1, color=AO3_ACCENT_RED, transform=ax.transAxes))\n",
        "\n",
        "# Bottom rating-inspired strip\n",
        "for i, color in enumerate(RATING_COLORS):\n",
        "    ax.add_patch(plt.Rectangle((i*0.25, 0), 0.25, 0.05, color=color, transform=ax.transAxes))\n",
        "\n",
        "# --- 5. Header (Using add_text_locked) ---\n",
        "add_text_locked(fig, 0.08, 0.86, f\"{YEAR_LIMIT}: A YEAR IN THE ARCHIVE\",\n",
        "                fontsize=48, fontweight=\"bold\", color=AO3_TEXT_DARK, ha=\"left\")\n",
        "\n",
        "add_text_locked(fig, 0.08, 0.8, \"PERSONAL READING JOURNEY\",\n",
        "                fontsize=32, color=AO3_TEXT_DARK, ha=\"left\",\n",
        "                alpha=0.6)\n",
        "\n",
        "# --- 6. Core Stats (High Impact) ---\n",
        "# Word Count\n",
        "add_text_locked(fig, 0.5, 0.66, f\"{total_words}\",\n",
        "                ha=\"center\", fontsize=72, fontweight=\"bold\", color=AO3_ACCENT_RED)\n",
        "add_text_locked(fig, 0.5, 0.6, \"WORDS VISITED\",\n",
        "                ha=\"center\", fontsize=24, color=AO3_TEXT_DARK,\n",
        "                alpha=0.6, fontweight=\"semibold\")\n",
        "\n",
        "# Works Count\n",
        "add_text_locked(fig, 0.5, 0.49, f\"{total_works}\",\n",
        "                ha=\"center\", fontsize=72, fontweight=\"bold\", color=AO3_ACCENT_RED)\n",
        "add_text_locked(fig, 0.5, 0.43, \"WORKS EXPLORED\",\n",
        "                ha=\"center\", fontsize=24, color=AO3_TEXT_DARK,\n",
        "                alpha=0.6, fontweight=\"semibold\")\n",
        "\n",
        "# Divider Line\n",
        "fig.lines.append(plt.Line2D([0.15, 0.85], [0.46, 0.46], lw=1.5, color=AO3_OTHER_GRAY, alpha=0.5))\n",
        "\n",
        "# --- 7. Flavor Stats (Structured Layout) ---\n",
        "# Left Column: Longest Fic\n",
        "add_text_locked(fig, 0.15, 0.315, \"LONGEST JOURNEY\", ha=\"left\", fontsize=22,\n",
        "                color=AO3_TEXT_DARK, alpha=0.6, fontweight=\"bold\")\n",
        "add_text_locked(fig, 0.15, 0.27, f\"{longest_fic_words}\", ha=\"left\", fontsize=32, fontweight=\"bold\", color=AO3_TEXT_DARK)\n",
        "add_text_locked(fig, 0.15, 0.225, \"WORDS IN A SINGLE WORK\", ha=\"left\", fontsize=20,\n",
        "                color=AO3_TEXT_DARK, alpha=0.6)\n",
        "\n",
        "# Right Column: Oldest Fic\n",
        "add_text_locked(fig, 0.85, 0.315, \"ARCHIVAL FIND\", ha=\"right\", fontsize=22,\n",
        "                color=AO3_TEXT_DARK, alpha=0.6, fontweight=\"bold\")\n",
        "add_text_locked(fig, 0.85, 0.27, oldest_fic_year.upper(), ha=\"right\", fontsize=32, fontweight=\"bold\", color=AO3_TEXT_DARK)\n",
        "add_text_locked(fig, 0.85, 0.225, \"OLDEST WORK VISITED\", ha=\"right\", fontsize=20,\n",
        "                color=AO3_TEXT_DARK, alpha=0.6)\n",
        "\n",
        "# --- 8. Footer ---\n",
        "add_text_locked(fig, 0.5, 0.09, \"BASED ON WORKS YOU OPENED THIS YEAR\",\n",
        "                ha=\"center\", fontsize=14, color=AO3_TEXT_DARK, alpha=0.5)\n",
        "\n",
        "# The \"Pro\" touch\n",
        "add_text_locked(fig, 0.95, 0.025, f\"AO3 WRAPPED @ {YEAR_LIMIT}\",\n",
        "                ha=\"right\", fontsize=12, color=AO3_BG_LIGHT, alpha=0.8)\n",
        "\n",
        "# --- 9. Save\n",
        "\n",
        "# --- Save & Show ---\n",
        "plt.savefig(\"ao3_wrapped_cover.png\", dpi=300, bbox_inches=\"tight\", facecolor=AO3_BG_LIGHT)\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "8HSWsIbaOVRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 2: Daily Work Opened Heatmap"
      ],
      "metadata": {
        "id": "W2ioXCHZgW4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Daily Work Opened Heatmap\n",
        "\n",
        "daily_works = get_daily_data(df, value_col='work_id', agg_func='nunique')\n",
        "daily_works_ts = reindex_time_series(daily_works)\n",
        "\n",
        "# --- 1. Create the Pivot Table ---\n",
        "\n",
        "# Rows: day of week (0=Mon to 6=Sun)\n",
        "# Columns: week of year\n",
        "# Values: the daily count of works finished ('daily_metric')\n",
        "heatmap_data = daily_works_ts.pivot_table(\n",
        "    index='dayofweek',\n",
        "    columns='weekofyear',\n",
        "    values='daily_metric',\n",
        "    aggfunc='sum' # Use sum, though it should just be one value per cell\n",
        ")\n",
        "\n",
        "# Fill any NaN values (should be none if reindexing was done, but safe to keep)\n",
        "heatmap_data = heatmap_data.fillna(0)\n",
        "\n",
        "\n",
        "# --- 2. Reorder Rows for Standard Calendar View ---\n",
        "\n",
        "# By default, Pandas dayofweek is Monday=0 to Sunday=6.\n",
        "# For a typical calendar view (Monday at the top, Sunday at the bottom),\n",
        "# we need to reverse the order of the rows.\n",
        "\n",
        "# Note: heatmap_data.iloc[::-1] reverses the rows.\n",
        "heatmap_data = heatmap_data.iloc[::-1]\n",
        "\n",
        "\n",
        "# --- 3. Define Weekday Labels (Reordered) ---\n",
        "\n",
        "# Define the labels to match the reversed order:\n",
        "weekday_labels = ['Sun', 'Sat', 'Fri', 'Thu', 'Wed', 'Tue', 'Mon']\n",
        "\n",
        "# 1. Prepare data for Mon-Sun order\n",
        "# heatmap_data is currently Sun-Mon (due to iloc[::-1] in 4oO0RtpobUUf)\n",
        "# Reverse it again to get Mon-Sun for display\n",
        "heatmap_data_mon_sun = heatmap_data.iloc[::-1]\n",
        "weekday_labels_mon_sun = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "\n",
        "# --- 1. Figure Setup and Plotting ---\n",
        "plt.figure(figsize=(20, 4)) # Wide and short for the calendar look\n",
        "\n",
        "ax = sns.heatmap(\n",
        "    heatmap_data_mon_sun, # Use the new data with Mon-Sun order\n",
        "    cmap=AO3_HEATMAP_CMAP,\n",
        "    linewidths=2,                   # Increased line width for cell borders\n",
        "    linecolor=AO3_BG_LIGHT,         # White lines between cells\n",
        "    square=True,                    # Ensure cells are square\n",
        "    cbar=True,                      # CRITICAL: Add the color bar\n",
        "    cbar_kws={\n",
        "        'label': 'Works Opened',\n",
        "        'shrink': 0.72,\n",
        "        'pad': 0.015}, # Label for the color bar\n",
        "    yticklabels=weekday_labels_mon_sun,     # Use the new labels for Mon-Sun\n",
        "    xticklabels=False,              # Remove week numbers\n",
        "    vmin=0                          # Ensure 0 starts at the lightest color\n",
        ")\n",
        "\n",
        "# Reduce colorbar label font size\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.set_label('Works Finished', fontsize=10)\n",
        "cbar.ax.tick_params(labelsize=9)\n",
        "\n",
        "# --- 2. Axis and Tick Cleanup (Minimalism) ---\n",
        "\n",
        "# Remove axis labels and tick marks\n",
        "ax.set_xlabel('')\n",
        "ax.set_ylabel('')\n",
        "ax.tick_params(axis='x', length=0)\n",
        "ax.tick_params(axis='y', length=0)\n",
        "\n",
        "# Rotate Y-labels to be horizontal and align correctly\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# --- 3. Adding Month Annotations ---\n",
        "\n",
        "# The utility function needs the index of the original daily data (before pivot)\n",
        "# to correctly calculate week numbers relative to the start of the year.\n",
        "# Assuming 'daily_works_ts' is the DataFrame from reindex_time_series.\n",
        "first_day_of_data = daily_works_ts.index.min()\n",
        "last_day_of_data = daily_works_ts.index.max()\n",
        "\n",
        "# Recalculate full date range needed for annotation logic\n",
        "date_index_for_ann = pd.date_range(start=first_day_of_data, end=last_day_of_data, freq='D')\n",
        "\n",
        "# Get month positions and labels\n",
        "month_starts, month_labels = get_month_annotations(date_index_for_ann)\n",
        "\n",
        "# Place the month labels above the weeks they start in\n",
        "for pos, label in zip(month_starts, month_labels):\n",
        "    ax.text(pos,\n",
        "            len(weekday_labels_mon_sun) + 0.8,\n",
        "            label, ha='left', va='bottom', fontsize=12, color=AO3_TEXT_DARK)\n",
        "\n",
        "\n",
        "# --- 4. Final Title and Layout ---\n",
        "\n",
        "# Title (aligned left, using locked font)\n",
        "add_text_locked(\n",
        "    obj=ax,\n",
        "    x=0,             # x position is ignored for titles\n",
        "    y=0,             # y position is ignored for titles\n",
        "    text='THE DAILY PULSE: FICS OPENED PER DAY',\n",
        "    fontsize=TITLE_FONT_SIZE,\n",
        "    fontweight='bold',\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',       # Title alignment: left, center, right\n",
        "    is_title=True,\n",
        "    pad=PADDING\n",
        ")\n",
        "\n",
        "# # Subtitle (positioned above the plot, axes coordinates)\n",
        "ax.text(\n",
        "    0, 1.08,  # Position: x=0 (left-aligned), y=1.08 (above the plot)\n",
        "    'Your reading curiosity, traced across the calendar one day at a time.',\n",
        "    transform=ax.transAxes,  # Use axes coordinates (0-1 range)\n",
        "    fontsize=SUBTITLE_FONT_SIZE,\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',  # Horizontal alignment\n",
        "    va='bottom'  # Vertical alignment\n",
        ")\n",
        "\n",
        "# Ensure plot limits are clean and axis scaling is correct\n",
        "# For Mon-Sun order, day 0 (Mon) should be at the top of the y-axis (index 0 in heatmap_data_mon_sun)\n",
        "ax.set_ylim(len(weekday_labels_mon_sun), 0) # Corrected y-axis limit for Mon at top\n",
        "ax.set_xlim(0, len(heatmap_data.columns)) # Week columns\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- 5. Save High-Resolution Output ---\n",
        "plt.savefig(\n",
        "    'daily_works_heatmap.png',\n",
        "    dpi=OUTPUT_DPI,\n",
        "    bbox_inches='tight',\n",
        "    facecolor='white'\n",
        ")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4oO0RtpobUUf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 3: Deleted Works"
      ],
      "metadata": {
        "id": "TZm2WG5_bVgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Monthly Deleted Work Bar Chart\n",
        "\n",
        "# --- Handle empty deleted work DataFrame ---\n",
        "if df_del.empty or df_del['last_visited_date'].dropna().empty:\n",
        "    # Create an \"empty\" plotting series for consistency (0 bars for 12 months)\n",
        "    import datetime\n",
        "    today = pd.Timestamp.today()\n",
        "    months = pd.period_range(today - pd.DateOffset(months=11), today, freq='M')\n",
        "    plotting_series = pd.Series(0, index=months.to_timestamp())\n",
        "\n",
        "    # Base gray bars for empty plot\n",
        "    BASE_GRAY = '#C0C0C0'\n",
        "    bar_colors = [BASE_GRAY] * len(plotting_series)\n",
        "\n",
        "    # Create the figure\n",
        "    plt.figure(figsize=(14, 6), facecolor=AO3_BG_LIGHT)\n",
        "    ax = plt.gca()\n",
        "    ax.bar(plotting_series.index, plotting_series.values, color=bar_colors, width=19)\n",
        "\n",
        "    # Title\n",
        "    TITLE_TEXT = 'GHOST READING HISTORY: DELETED WORKS PER MONTH'\n",
        "    add_text_locked(\n",
        "        obj=ax,\n",
        "        x=0,\n",
        "        y=0,\n",
        "        text=TITLE_TEXT,\n",
        "        fontsize=TITLE_FONT_SIZE,\n",
        "        fontweight='bold',\n",
        "        color=AO3_TEXT_DARK,\n",
        "        ha='left',\n",
        "        is_title=True,\n",
        "        pad=PADDING\n",
        "    )\n",
        "\n",
        "    # Fun / interesting subtitle\n",
        "    ax.text(\n",
        "        0, 1.06,\n",
        "        \"Good news! You haven't lost any works yet â€” your archive remains pristine.\",\n",
        "        transform=ax.transAxes,\n",
        "        ha='left',\n",
        "        va='bottom',\n",
        "        fontsize=SUBTITLE_FONT_SIZE,\n",
        "        color=AO3_TEXT_DARK,\n",
        "        wrap=True\n",
        "    )\n",
        "\n",
        "    # Labels & formatting\n",
        "    ax.set_xlabel('Month Read', fontsize=14, color=AO3_TEXT_DARK)\n",
        "    ax.set_ylabel('Number of Deleted Works', fontsize=14, color=AO3_TEXT_DARK)\n",
        "    ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b'))\n",
        "    sns.despine(ax=ax, top=True, right=True)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.5, color=AO3_GRID_LIGHT)\n",
        "    ax.set_ylim(0, 5)  # Small fixed y-range to show minimal bars\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save high-resolution PNG\n",
        "    plt.savefig('deleted_works_per_month.png', dpi=OUTPUT_DPI, bbox_inches='tight', facecolor=AO3_BG_LIGHT)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # --- Existing plotting code for non-empty df_del ---\n",
        "    df_del['last_visited_date'] = pd.to_datetime(df_del['last_visited_date'])\n",
        "    df_del['month'] = df_del['last_visited_date'].dt.to_period('M')\n",
        "    monthly_deleted_counts = df_del.groupby('month').size()\n",
        "    monthly_deleted_counts.name = 'deleted_work_count'\n",
        "\n",
        "    full_month_range = pd.period_range(\n",
        "        start=monthly_deleted_counts.index.min(),\n",
        "        end=monthly_deleted_counts.index.max(),\n",
        "        freq='M'\n",
        "    )\n",
        "    monthly_deleted_counts = monthly_deleted_counts.reindex(full_month_range, fill_value=0)\n",
        "    plotting_series = monthly_deleted_counts.to_timestamp()\n",
        "\n",
        "    max_deleted_count = plotting_series.max()\n",
        "    max_month_index = plotting_series.idxmax()\n",
        "    max_deleted_month = max_month_index.strftime('%b %Y')\n",
        "\n",
        "    BASE_GRAY = '#C0C0C0'\n",
        "    bar_colors = [\n",
        "        AO3_ACCENT_RED if date_index == max_month_index else BASE_GRAY\n",
        "        for date_index in plotting_series.index\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(14, 6), facecolor=AO3_BG_LIGHT)\n",
        "    ax = plt.gca()\n",
        "    ax.bar(plotting_series.index, plotting_series.values, color=bar_colors, width=19)\n",
        "\n",
        "    TITLE_TEXT = 'GHOST READING HISTORY: DELETED WORKS PER MONTH'\n",
        "    add_text_locked(\n",
        "        obj=ax,\n",
        "        x=0,\n",
        "        y=0,\n",
        "        text=TITLE_TEXT,\n",
        "        fontsize=TITLE_FONT_SIZE,\n",
        "        fontweight='bold',\n",
        "        color=AO3_TEXT_DARK,\n",
        "        ha='left',\n",
        "        is_title=True,\n",
        "        pad=PADDING\n",
        "    )\n",
        "\n",
        "    subtitle_text = (\n",
        "        f\"The most tragic loss occurred in {max_deleted_month}, \"\n",
        "        f\"where {max_deleted_count} works vanished from the archive.\"\n",
        "    )\n",
        "    ax.text(0, 1.06, subtitle_text, transform=ax.transAxes, ha='left', va='bottom', fontsize=SUBTITLE_FONT_SIZE, color=AO3_TEXT_DARK, wrap=True)\n",
        "\n",
        "    # ax.set_xlabel('Month Read', fontsize=14, color=AO3_TEXT_DARK)\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel('Number of Deleted Works', fontsize=14, color=AO3_TEXT_DARK)\n",
        "    ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%b'))\n",
        "    sns.despine(ax=ax, top=True, right=True)\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.5, color=AO3_GRID_LIGHT)\n",
        "    ax.set_ylim(bottom=0)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig('deleted_works_per_month.png', dpi=OUTPUT_DPI, bbox_inches='tight', facecolor=AO3_BG_LIGHT)\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jSlL85E3Z5-n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 4: Top Fandoms Stream Chart"
      ],
      "metadata": {
        "id": "bo-Lca9RZkDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evolution of Interests: Top Fandoms over Time\n",
        "\n",
        "# Define OUTPUT_DPI if not already in your config\n",
        "# OUTPUT_DPI = 300\n",
        "\n",
        "NUM_TOP_FANDOMS = 8\n",
        "\n",
        "def plot_proportional_stream_chart(\n",
        "    long_df,\n",
        "    category_label='Fandom',\n",
        "    num_top=NUM_TOP_FANDOMS,\n",
        "    save_path=\"fandom_stream_chart.png\"\n",
        "):\n",
        "    df_temp = long_df.copy()\n",
        "\n",
        "    # --- 1. Preparation ---\n",
        "    df_temp['full_read_date'] = pd.to_datetime(df_temp['full_read_date'])\n",
        "    df_temp['month'] = df_temp['full_read_date'].dt.to_period('M')\n",
        "\n",
        "    top_categories = df_temp['attribute_value'].value_counts().head(num_top).index.tolist()\n",
        "    df_filtered = df_temp[df_temp['attribute_value'].isin(top_categories)].copy()\n",
        "\n",
        "    monthly_counts = df_filtered.groupby(['month', 'attribute_value']).size().unstack(fill_value=0)\n",
        "    monthly_counts = monthly_counts[top_categories]\n",
        "    pivot_norm = monthly_counts.div(monthly_counts.sum(axis=1), axis=0).fillna(0)\n",
        "\n",
        "    # --- 2. Narrative Logic ---\n",
        "    start_val = pivot_norm.iloc[0].idxmax()\n",
        "    end_val = pivot_norm.iloc[-1].idxmax()\n",
        "\n",
        "    mapping = {\"Original Work\": \"Published Fiction\"}\n",
        "    start_disp = mapping.get(start_val, start_val)\n",
        "    end_disp = mapping.get(end_val, end_val)\n",
        "\n",
        "    TITLE_TEXT = f\"EVOLUTION OF INTERESTS: TOP {category_label.upper()}S IN {YEAR_LIMIT}\"\n",
        "    if start_val == end_val:\n",
        "        SUBTITLE_TEXT = f\"A portrait of loyalty: {start_disp} remained your defining interest throughout this period.\"\n",
        "    else:\n",
        "        SUBTITLE_TEXT = f\"Tracing the shift from your {start_disp} era to your current {end_disp} fixation.\"\n",
        "\n",
        "    # --- 3. Plotting ---\n",
        "    # We increase the figure size slightly for better resolution handling\n",
        "    fig, ax = plt.subplots(figsize=(18, 9), facecolor='#FFFFFF')\n",
        "\n",
        "    colors = sns.color_palette(\"Set2\", n_colors=len(top_categories))\n",
        "    pivot_norm.plot(kind='area', stacked=True, ax=ax, color=colors, alpha=0.7, linewidth=0)\n",
        "\n",
        "    # Titles and Subtitles\n",
        "    add_text_locked(\n",
        "        obj=ax,\n",
        "        x=0,\n",
        "        y=0,\n",
        "        text=TITLE_TEXT,\n",
        "        fontsize=TITLE_FONT_SIZE,\n",
        "        fontweight='bold',\n",
        "        color=AO3_TEXT_DARK,\n",
        "        ha='left',\n",
        "        is_title=True,\n",
        "        pad=PADDING\n",
        "    )\n",
        "    ax.text(x=0, y=1.03, s=SUBTITLE_TEXT, transform=ax.transAxes,\n",
        "            fontsize=SUBTITLE_FONT_SIZE, fontweight='medium', color='#333333', ha='left', va='bottom')\n",
        "\n",
        "    # Formatting\n",
        "    # Ensure the axis has room to breathe without clipping labels\n",
        "    ax.margins(x=0.02) # Small 2% margin so the first/last labels have space\n",
        "\n",
        "    x_values = pivot_norm.index.to_timestamp()\n",
        "    month_labels = [m.strftime('%b') for m in pivot_norm.index]\n",
        "\n",
        "    ax.set_xticks(x_values)\n",
        "    ax.set_xticklabels(month_labels, rotation=0, fontsize=12)\n",
        "    ax.set_xlabel(\"\")\n",
        "\n",
        "    # Y-Axis: Percentages and Label\n",
        "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f'{int(x*100)}%'))\n",
        "    ax.set_ylabel(f\"Share of Top {num_top} {category_label}s\", fontsize=12)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    ax.legend(title=category_label, bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
        "    sns.despine(ax=ax, left=True, bottom=False)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "\n",
        "    # --- 4. Save and Show ---\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # # Save the figure BEFORE plt.show()\n",
        "    plt.savefig(\n",
        "        save_path,\n",
        "        dpi=OUTPUT_DPI,\n",
        "        bbox_inches='tight',\n",
        "        facecolor=fig.get_facecolor(),\n",
        "        transparent=False\n",
        "    )\n",
        "    # print(f\"Successfully saved chart to: {save_path} at {OUTPUT_DPI} DPI\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# --- Execution ---\n",
        "plot_proportional_stream_chart(\n",
        "    long_df=cleaned_data['fandoms'],\n",
        "    category_label='Fandom',\n",
        "    num_top=NUM_TOP_FANDOMS\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xeRu_K5XLPv1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 5: Favourite Fics and Authors"
      ],
      "metadata": {
        "id": "BfB9aY8D9OYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Favourite Fics and Authors\n",
        "\n",
        "# ---------- Data Preparation ----------\n",
        "\n",
        "# 1. Top 5 Visited Works\n",
        "df_top_works = df.sort_values(by='visited_count', ascending=False).head(5).copy()\n",
        "\n",
        "# 2. Top 5 Authors (Excluding Aggregators)\n",
        "exclude_authors = ['anonymous', 'orphan_account', 'Anonymous']\n",
        "df_filtered_authors = df[~df['author'].isin(exclude_authors)].copy()\n",
        "df_top_authors = df_filtered_authors.groupby('author')['work_id'].nunique().sort_values(ascending=False).head(5).reset_index()\n",
        "df_top_authors.columns = ['author', 'unique_works_count']\n",
        "\n",
        "# Stats for the technical note\n",
        "anon_count = df[df['author'].str.lower() == 'anonymous']['work_id'].nunique()\n",
        "orphan_count = df[df['author'] == 'orphan_account']['work_id'].nunique()\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "# Define two specific shades of AO3 Red\n",
        "RED_DEEP = '#990000'   # Classic AO3 Red\n",
        "RED_VIVID = '#D00000'  # Slightly brighter Red\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 9), facecolor=AO3_BG_LIGHT)\n",
        "\n",
        "# --- Subplot 1: Top 5 Visited Works ---\n",
        "ax1 = axes[0]\n",
        "sns.barplot(data=df_top_works, x='visited_count', y='title', ax=ax1, color=RED_DEEP)\n",
        "add_text_locked(\n",
        "    obj=ax1,\n",
        "    x=0,\n",
        "    y=0,\n",
        "    text='MOST VISITED WORKS',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',\n",
        "    is_title=True,\n",
        "    pad=10\n",
        ")\n",
        "ax1.set_xlabel('Total Visits', fontsize=10, color=AO3_TEXT_DARK)\n",
        "ax1.set_ylabel('') # Clean Y-axis\n",
        "\n",
        "# --- Subplot 2: Top 5 Authors ---\n",
        "ax2 = axes[1]\n",
        "sns.barplot(data=df_top_authors, x='unique_works_count', y='author', ax=ax2, color=RED_VIVID)\n",
        "add_text_locked(\n",
        "    obj=ax2,\n",
        "    x=0,\n",
        "    y=0,\n",
        "    text='MOST VISITED AUTHORS',\n",
        "    fontsize=14,\n",
        "    fontweight='bold',\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',\n",
        "    is_title=True,\n",
        "    pad=10\n",
        ")\n",
        "ax2.set_xlabel('Unique Works Opened', fontsize=10, color=AO3_TEXT_DARK)\n",
        "ax2.set_ylabel('')\n",
        "\n",
        "# --- Global Styling & Annotations ---\n",
        "for ax in axes:\n",
        "    sns.despine(ax=ax, left=True)\n",
        "    ax.tick_params(axis='both', colors=AO3_TEXT_DARK)\n",
        "\n",
        "    # Place count number INSIDE the bar in white\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, label_type='center', color='white',\n",
        "                     fontweight='bold', fontsize=12)\n",
        "\n",
        "# --- Integrated Titles & Narratives (Matching WordCloud Style) ---\n",
        "# General Title\n",
        "add_text_locked(\n",
        "    obj=fig,\n",
        "    x=0.08, y=0.96,\n",
        "    text='FAVOURITE FICS: TOP WORKS & AUTHORS',\n",
        "    fontsize=TITLE_FONT_SIZE, fontweight='bold', color=AO3_TEXT_DARK, ha='left'\n",
        ")\n",
        "\n",
        "# Narrative Subtitle\n",
        "top_work = df_top_works.iloc[0]['title']\n",
        "top_auth = df_top_authors.iloc[0]['author']\n",
        "subtitle_text = (\n",
        "    \"If only you could leave multiple kudos on a single fic! Maybe leave a comment to let the authors know you appreciate their work?\"\n",
        ")\n",
        "fig.text(\n",
        "    x=0.08, y=0.91,\n",
        "    s=subtitle_text,\n",
        "    fontsize=SUBTITLE_FONT_SIZE, color='#000000', ha='left'\n",
        ")\n",
        "\n",
        "# --- Build dynamic note text ---\n",
        "parts = []\n",
        "\n",
        "if anon_count > 0:\n",
        "    parts.append(f\"Anonymous accounts ({anon_count} work{'s' if anon_count != 1 else ''})\")\n",
        "\n",
        "if orphan_count > 0:\n",
        "    parts.append(f\"Orphan accounts ({orphan_count} work{'s' if orphan_count != 1 else ''})\")\n",
        "\n",
        "if parts:  # Only show note if there is at least one category\n",
        "    if len(parts) == 1:\n",
        "        note_intro = parts[0] + \" doesnâ€™t appear in the top author list.\"\n",
        "    else:\n",
        "        note_intro = \" and \".join(parts) + \" donâ€™t appear in the top author list.\"\n",
        "\n",
        "    note_text = note_intro + \" But we still thank the anons for their brilliant works.\"\n",
        "\n",
        "    fig.text(\n",
        "        0.5, 0.02,\n",
        "        note_text,\n",
        "        ha='center',\n",
        "        fontsize=14,\n",
        "        color=AO3_TEXT_DARK,\n",
        "        style='italic',\n",
        "        alpha=0.6\n",
        "    )\n",
        "# If both counts are 0, nothing is displayed\n",
        "\n",
        "\n",
        "plt.tight_layout(rect=[0.05, 0.05, 0.95, 0.90]) # Padding for titles\n",
        "plt.savefig(\"top_works_authors.png\", dpi=OUTPUT_DPI, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N4KcxVCh4EBn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 6: Top Fandoms"
      ],
      "metadata": {
        "id": "2-JGGNFpjyb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top Fandoms Word Cloud\n",
        "from matplotlib import font_manager, colormaps\n",
        "\n",
        "# ---------- Process Data using the Long-Form Utility ----------\n",
        "# Generate frequency dictionary from the cleaned long-form data\n",
        "fandom_counts = cleaned_data['fandoms']['attribute_value'].value_counts().to_dict()\n",
        "\n",
        "# Create a new colormap using only the 25% to 100% range of RdPu\n",
        "colors = plt.get_cmap('Blues')(np.linspace(0.25, 1.0, 256))\n",
        "new_cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "# Generate\n",
        "fandom_wordcloud = generate_wc(fandom_counts, colors=new_cmap)\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "plt.figure(figsize=(16, 8), facecolor=AO3_BG_LIGHT)\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.imshow(fandom_wordcloud, interpolation='bilinear')\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# Title\n",
        "add_text_locked(\n",
        "    obj=ax,\n",
        "    x=0,            # ignored for titles\n",
        "    y=0,            # ignored for titles\n",
        "    text='FANDOM UNIVERSE: YOUR READING LANDSCAPE',\n",
        "    fontsize=TITLE_FONT_SIZE,\n",
        "    fontweight='bold',\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',      # maps to ax.set_title(loc='left')\n",
        "    is_title=True,\n",
        "    pad=PADDING\n",
        ")\n",
        "\n",
        "# Narrative Subtitle\n",
        "top_fandoms = sorted(fandom_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "top_fandoms_str = ', '.join([f[0] for f in top_fandoms])\n",
        "\n",
        "# Count total works\n",
        "total_works = len(df)\n",
        "\n",
        "ax.text(\n",
        "    x=0, y=1.04,\n",
        "    s=f\"Across {total_works} works, your heart belongs to: {top_fandoms_str}.\",\n",
        "    transform=ax.transAxes, ha='left', va='bottom', fontsize=SUBTITLE_FONT_SIZE,\n",
        "    color='#000000'\n",
        ")\n",
        "\n",
        "# Save high-res\n",
        "plt.savefig(\"fandom_wordcloud.png\", dpi=OUTPUT_DPI, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xWXZZq7Vj14w",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 7: Top Characters"
      ],
      "metadata": {
        "id": "3egI7WLYaNUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top Characters Word Cloud\n",
        "from matplotlib import font_manager, colormaps\n",
        "\n",
        "# ---------- Process Data using the Long-Form Utility ----------\n",
        "# Generate frequency dictionary from the cleaned long-form data\n",
        "char_counts = cleaned_data['characters']['attribute_value'].value_counts().to_dict()\n",
        "\n",
        "# Create a new colormap using only the 25% to 100% range of RdPu\n",
        "colors = plt.get_cmap('Purples')(np.linspace(0.25, 1.0, 256))\n",
        "new_cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "\n",
        "# Generate\n",
        "character_wordcloud = generate_wc(char_counts, colors=new_cmap)\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "plt.figure(figsize=(16, 8), facecolor=AO3_BG_LIGHT)\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.imshow(character_wordcloud, interpolation='bilinear')\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# Title\n",
        "add_text_locked(\n",
        "    obj=ax,\n",
        "    x=0,            # ignored for titles\n",
        "    y=0,            # ignored for titles\n",
        "    text='CHARACTER POWER RANKINGS: YOUR MOST READ',\n",
        "    fontsize=TITLE_FONT_SIZE,\n",
        "    fontweight='bold',\n",
        "    color=AO3_TEXT_DARK,\n",
        "    ha='left',      # maps to ax.set_title(loc='left')\n",
        "    is_title=True,\n",
        "    pad=PADDING\n",
        ")\n",
        "\n",
        "# Narrative Subtitle\n",
        "top_5 = list(character_wordcloud.words_.keys())[:5]\n",
        "top_str = ', '.join(top_5)\n",
        "ax.text(\n",
        "    x=0, y=1.04, s=f\"Your reading focus revolves heavily around: {top_str}.\",\n",
        "    transform=ax.transAxes, ha='left', va='bottom', fontsize=SUBTITLE_FONT_SIZE,\n",
        "    color='#000000'\n",
        ")\n",
        "\n",
        "# Save high-res\n",
        "plt.savefig(\"character_wordcloud.png\", dpi=OUTPUT_DPI, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R7mdJlmue4Et",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization 8: Top Freeform Tags (By Rating)"
      ],
      "metadata": {
        "id": "6u0NxC-imGiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Top Freeforms Word Cloud\n",
        "from matplotlib import font_manager, colormaps\n",
        "\n",
        "# --- Configuration ---\n",
        "# AO3 Yellow: #ead70f\n",
        "# Create a custom gradient for Yellow to ensure it doesn't vanish on white\n",
        "yellow_colors = [\"#FFF394\", \"#FFE761\", \"#DAA520\"] # Light -> Icon Color -> Dark\n",
        "yellow_cmap = mcolors.LinearSegmentedColormap.from_list(\"AO3_Yellow\", yellow_colors)\n",
        "\n",
        "RATINGS = [\n",
        "    'General Audiences',\n",
        "    'Teen And Up Audiences',\n",
        "    'Mature',\n",
        "    'Explicit',\n",
        "    'Not Rated'\n",
        "    ]\n",
        "\n",
        "RATING_CMAPS = {\n",
        "    'Explicit': 'Reds',\n",
        "    'Mature': 'Oranges',\n",
        "    'Teen And Up Audiences': yellow_cmap,\n",
        "    'General Audiences': 'Greens',\n",
        "    'Not Rated': 'Greys'\n",
        "}\n",
        "\n",
        "# --- Execution ---\n",
        "tags_df = cleaned_data['freeforms'].copy()\n",
        "\n",
        "for rating in RATINGS:\n",
        "    # 1. Get frequency dict for this specific rating\n",
        "    # We filter the long-form tags by the rating column\n",
        "    rating_counts = tags_df[tags_df['rating'] == rating]['attribute_value'].value_counts().to_dict()\n",
        "\n",
        "    if not rating_counts:\n",
        "        print(f\"Skipping {rating}: No tags found.\")\n",
        "        continue\n",
        "\n",
        "    # 2. Create the cropped colormap (0.25 to 1.0 to ensure readability)\n",
        "    base_cmap_name = RATING_CMAPS.get(rating, 'Blues')\n",
        "    colors = plt.get_cmap(base_cmap_name)(np.linspace(0.25, 1.0, 256))\n",
        "    rating_cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    # 3. Generate Word Cloud using your existing helper\n",
        "    # Assuming your generate_wc takes (freq_dict, colormap=...)\n",
        "    wc = generate_wc(rating_counts, colors=rating_cmap)\n",
        "\n",
        "    # 4. Plotting\n",
        "    plt.figure(figsize=(16, 8), facecolor=AO3_BG_LIGHT)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    ax.imshow(wc, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Dynamic Title\n",
        "    add_text_locked(\n",
        "        obj=ax,\n",
        "        x=0,            # ignored for titles\n",
        "        y=0,            # ignored for titles\n",
        "        text=f'TAG LANDSCAPE: {rating.upper()}',\n",
        "        fontsize=TITLE_FONT_SIZE,\n",
        "        fontweight='bold',\n",
        "        color=AO3_TEXT_DARK,\n",
        "        ha='left',      # maps to ax.set_title(loc='left')\n",
        "        is_title=True,\n",
        "        pad=PADDING\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Subtitle with top 5 tags for this rating\n",
        "    top_tags = list(wc.words_.keys())[:5]\n",
        "    top_tags_str = ', '.join(top_tags)\n",
        "\n",
        "    ax.text(\n",
        "        x=0, y=1.04,\n",
        "        s=f\"Common themes in your {rating} reads: {top_tags_str}.\",\n",
        "        transform=ax.transAxes, ha='left', va='bottom',\n",
        "        fontsize=SUBTITLE_FONT_SIZE, color='#333333'\n",
        "    )\n",
        "\n",
        "    # 5. Save and Show\n",
        "    file_name = f\"wordcloud_rating_{rating.replace(' ', '_').lower()}.png\"\n",
        "    plt.savefig(file_name, dpi=OUTPUT_DPI, bbox_inches='tight', facecolor=AO3_BG_LIGHT)\n",
        "\n",
        "    # print(f\"Generated Word Cloud for: {rating}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xh7US17FmM7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ‰ Thatâ€™s the end of your AO3 Wrapped!\n",
        "\n",
        "Thanks for taking a tour through your reading history.  \n",
        "Your AO3 Wrapped captures the stories, fandoms, and moments that shaped your reading journey, highlighting patterns you might not have noticed before.\n",
        "\n",
        "Whether you stop here or keep exploring, this Wrapped is yours to enjoy.\n",
        "\n",
        "**Everything below is optional** â€” consider it bonus content if youâ€™re in the mood to explore further.\n",
        "\n",
        "\n",
        "&nbsp;  \n",
        "&nbsp;  \n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Want to Customize? *(Optional)*\n",
        "\n",
        "You can tweak how this Wrapped is generated â€” for example:\n",
        "\n",
        "- ğŸ“… Change the analysis year (`YEAR_LIMIT`)\n",
        "- ğŸ·ï¸ Adjust how many fandoms are shown (`NUM_TOP_FANDOMS`)\n",
        "- â±ï¸ Control scraping limits and delays\n",
        "\n",
        "### How to customize\n",
        "1. **Scroll back up** to the **SETUP** cell  \n",
        "2. Click **â€œShow codeâ€** to expand it  \n",
        "3. Find the section labeled **`CONFIGURATION SETTINGS`**  \n",
        "4. Edit the values you want  \n",
        "5. Run the cell again to apply your changes\n",
        "\n",
        "&nbsp;  \n",
        "&nbsp;  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Still Want to Dig Deeper? *(Optional)*\n",
        "\n",
        "The following cells let you **filter and zoom in** on specific parts of your reading history, such as:\n",
        "\n",
        "- A single fandom\n",
        "- A particular language\n",
        "- Specific characters or relationships\n",
        "- Custom time ranges\n",
        "\n",
        "Youâ€™ll find a few examples showing how to use the filter functions and how to generate visualizations from the filtered DataFrames.  \n",
        "These steps are completely optional â€” feel free to skip them if youâ€™re happy with the overview above :)\n",
        "\n",
        "&nbsp;  \n",
        "&nbsp;  \n",
        "&nbsp;  \n"
      ],
      "metadata": {
        "id": "hTKBLNZxAqk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Filter Function\n",
        "def filter_data(\n",
        "    df,\n",
        "    ratings_to_include=None,\n",
        "    fandoms_to_include=None,\n",
        "    min_visited_count=0,\n",
        "    min_word_count=0,\n",
        "    languages_to_include=None # New parameter for language filter\n",
        "):\n",
        "    \"\"\"\n",
        "    Applies multiple filters (Rating, Fandom, Visited Count, Word Count, Language)\n",
        "    to the AO3 data in a single pass.\n",
        "    \"\"\"\n",
        "    current_df = df.copy() # Work on a copy to avoid side effects\n",
        "\n",
        "    # 1. Apply Rating Filter\n",
        "    if ratings_to_include:\n",
        "        current_df = current_df[current_df['rating'].isin(ratings_to_include)]\n",
        "\n",
        "    # 2. Apply Fandom Filter\n",
        "    if fandoms_to_include:\n",
        "        def has_fandom(fandom_list):\n",
        "            # Safely evaluate string representation of list if needed, then check\n",
        "            if isinstance(fandom_list, str):\n",
        "                try:\n",
        "                    fandoms_parsed = ast.literal_eval(fandom_list)\n",
        "                except (ValueError, SyntaxError):\n",
        "                    fandoms_parsed = []\n",
        "            else:\n",
        "                fandoms_parsed = fandom_list\n",
        "\n",
        "            return any(fandom in fandoms_to_include for fandom in fandoms_parsed)\n",
        "\n",
        "        current_df = current_df[current_df['fandoms'].apply(has_fandom)]\n",
        "\n",
        "    # 3. Apply Language Filter\n",
        "    if languages_to_include:\n",
        "        current_df = current_df[current_df['language'].isin(languages_to_include)]\n",
        "\n",
        "    # 4. Apply Threshold Filters\n",
        "    current_df = current_df[current_df['visited_count'] >= min_visited_count]\n",
        "    current_df = current_df[current_df['word_count'] >= min_word_count]\n",
        "\n",
        "    # Reindex current_df before returning\n",
        "    current_df = current_df.reset_index(drop=True)\n",
        "\n",
        "    return current_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "00tTWgrWTZYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Examples to use the Filter Function\n",
        "\"\"\"\n",
        "Run any of these examples, or create your own custom filters!\n",
        "\"\"\"\n",
        "\n",
        "# Example 1: Only General Audiences works (SFW)\n",
        "df_ga = filter_data(\n",
        "    df,\n",
        "    ratings_to_include=['General Audiences']\n",
        ")\n",
        "\n",
        "# Example 2: Works you re-read (visited 2+ times)\n",
        "df_rereads = filter_data(\n",
        "    df,\n",
        "    min_visited_count=2\n",
        ")\n",
        "\n",
        "# Example 3: Long fics only (50k+ words)\n",
        "df_longfic = filter_data(\n",
        "    df,\n",
        "    min_word_count=50000\n",
        ")\n",
        "\n",
        "# Example 4: English works only\n",
        "df_english = filter_data(\n",
        "    df,\n",
        "    languages_to_include=['English']\n",
        ")\n",
        "\n",
        "# Example 5: Combined filters - Teen+ English longfics\n",
        "df_custom = filter_data(\n",
        "    df,\n",
        "    ratings_to_include=['Teen And Up Audiences', 'General Audiences'],\n",
        "    languages_to_include=['English'],\n",
        "    min_word_count=20000\n",
        ")\n",
        "\n",
        "# ğŸ’¡ TIP: Use filtered DataFrames in visualizations!\n",
        "# For example, to see a wordcloud of only SFW characters:\n",
        "# character_wordcloud = generate_cleaned_wordcloud(df_ga, column_name='characters')"
      ],
      "metadata": {
        "id": "RKsNhfvbTtQG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Use Filtered Dataframes Examples - Character Stream Chart for English Works\n",
        "\n",
        "# --- Example: Character Stream Chart for English Works (df_english) ---\n",
        "\n",
        "# 1. Get work_ids from df_english\n",
        "english_work_ids = df_english['work_id'].unique()\n",
        "\n",
        "# 2. Filter cleaned_data['characters'] by these work_ids\n",
        "filtered_characters_english_df = cleaned_data['characters'][cleaned_data['characters']['work_id'].isin(english_work_ids)].copy()\n",
        "\n",
        "# 3. Plot the proportional stream chart for characters\n",
        "if not filtered_characters_english_df.empty:\n",
        "    plot_proportional_stream_chart(\n",
        "        long_df=filtered_characters_english_df,\n",
        "        category_label='Character',\n",
        "        num_top=NUM_TOP_FANDOMS, # Reusing NUM_TOP_FANDOMS for characters for now\n",
        "        save_path=\"characters_stream_chart_english.png\"\n",
        "    )\n",
        "else:\n",
        "    print(\"No character data found for English works to generate a stream chart.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mCs0dNutCATP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Use Filtered Dataframes Example - Relationship Word Cloud for Reread Works\n",
        "\n",
        "# --- Example: Relationship Word Cloud for Reread Works (df_rereads) ---\n",
        "\n",
        "# 1. Get work_ids from df_rereads\n",
        "reread_work_ids = df_rereads['work_id'].unique()\n",
        "\n",
        "# 2. Filter cleaned_data['relationships'] by these work_ids\n",
        "filtered_relationships_df = cleaned_data['relationships'][cleaned_data['relationships']['work_id'].isin(reread_work_ids)].copy()\n",
        "\n",
        "# 3. Generate frequency dictionary\n",
        "relationship_counts_rereads = filtered_relationships_df['attribute_value'].value_counts().to_dict()\n",
        "\n",
        "if not relationship_counts_rereads:\n",
        "    print(\"No relationship tags found for re-read works to generate a word cloud.\")\n",
        "else:\n",
        "    # 4. Create a new colormap (e.g., Reds, similar to other character/fandom WCs)\n",
        "    colors = plt.get_cmap('Reds')(np.linspace(0.25, 1.0, 256))\n",
        "    new_cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    # 5. Generate Word Cloud\n",
        "    relationship_wordcloud_rereads = generate_wc(relationship_counts_rereads, colors=new_cmap)\n",
        "\n",
        "    # 6. Plotting\n",
        "    plt.figure(figsize=(16, 8), facecolor=AO3_BG_LIGHT)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    ax.imshow(relationship_wordcloud_rereads, interpolation='bilinear')\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Title\n",
        "    add_text_locked(\n",
        "        obj=ax,\n",
        "        x=0,\n",
        "        y=0,\n",
        "        text='REREAD WORKS: RELATIONSHIP LANDSCAPE',\n",
        "        fontsize=TITLE_FONT_SIZE,\n",
        "        fontweight='bold',\n",
        "        color=AO3_TEXT_DARK,\n",
        "        ha='left',\n",
        "        is_title=True,\n",
        "        pad=PADDING\n",
        "    )\n",
        "\n",
        "    # Narrative Subtitle\n",
        "    top_5 = list(relationship_wordcloud_rereads.words_.keys())[:5]\n",
        "    top_str = ', '.join(top_5)\n",
        "    ax.text(\n",
        "        x=0, y=1.04, s=f\"In your re-reads, your character pairings of choice are: {top_str}.\",\n",
        "        transform=ax.transAxes, ha='left', va='bottom', fontsize=SUBTITLE_FONT_SIZE,\n",
        "        color='#000000'\n",
        "    )\n",
        "\n",
        "    # Save high-res\n",
        "    # plt.savefig(\"relationship_wordcloud_rereads.png\", dpi=OUTPUT_DPI, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ELwGfNKwCu2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;  \n",
        "&nbsp;  \n",
        "\n",
        "# Troubleshoot\n",
        "- If you get a TooManyRedirects error during scraping, run the cell below, check the status code and the link you're redirected to."
      ],
      "metadata": {
        "id": "mrEdjL6TrJDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Debug TooManyRedirects Error during Scraping\n",
        "# --- Redirect Diagnostic Tool ---\n",
        "test_url = f\"https://archiveofourown.org/users/{username}/readings\"\n",
        "\n",
        "# We set allow_redirects=False to see the VERY FIRST \"hop\"\n",
        "response = my_session.get(test_url, allow_redirects=False, timeout=15)\n",
        "\n",
        "print(f\"ğŸš© Initial Status Code: {response.status_code}\")\n",
        "if 'Location' in response.headers:\n",
        "    redirect_target = response.headers['Location']\n",
        "    print(f\"â¡ï¸ Redirecting to: {redirect_target}\")\n",
        "\n",
        "    if \"tos_acceptance\" in redirect_target:\n",
        "        print(\"\\nğŸ’¡ DIAGNOSIS: Terms of Service. Log in on your phone/PC and click 'I Agree'.\")\n",
        "    elif \"login\" in redirect_target:\n",
        "        print(\"\\nğŸ’¡ DIAGNOSIS: Session Rejected. Your '_otwarchive_session' cookie is likely expired or invalid.\")\n",
        "    elif \"cloudflare\" in redirect_target or response.status_code == 403:\n",
        "        print(\"\\nğŸ’¡ DIAGNOSIS: Cloudflare Block. Your home IP's clearance cookie won't work on Google's IP.\")\n",
        "else:\n",
        "    print(\"âœ… No redirect detected on the first hit.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NNXiY8P5rSo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;  \n",
        "&nbsp;  \n",
        "\n",
        "\n",
        "## GitHub Repo\n",
        "\n",
        "\n",
        "\n",
        "[![View on GitHub](https://img.shields.io/badge/GitHub-Source-blue?logo=github)](https://github.com/anulomufa/AO3-Wrapped)\n",
        "\n",
        "Feel free to watch, star, or fork this repository. You can also raise issues or contribute improvements via Pull Requests.\n"
      ],
      "metadata": {
        "id": "N-wSiEBLrinO"
      }
    }
  ]
}